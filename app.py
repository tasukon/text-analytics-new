import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from janome.tokenizer import Tokenizer
from collections import Counter
from wordcloud import WordCloud
import networkx as nx
import itertools
import re

# --- 1. ã‚¢ãƒ—ãƒªã®è¨­å®š ---
st.set_page_config(page_title="Text Analytics V9", layout="wide")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if 'df' not in st.session_state:
    st.session_state.df = None
if 'user_stopwords' not in st.session_state:
    st.session_state.user_stopwords = []

# åŸºæœ¬ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
DEFAULT_STOPWORDS = [
    "ã®", "ã«", "ã¯", "ã‚’", "ãŸ", "ãŒ", "ã§", "ã¦", "ã¨", "ã—", "ã‚Œ", "ã•",
    "ã‚ã‚‹", "ã„ã‚‹", "ã‚‚", "ã™ã‚‹", "ã‹ã‚‰", "ãª", "ã“ã¨", "ã¨ã—ã¦", "ã„", "ã‚„",
    "ã‚Œã‚‹", "ãªã©", "ãªã„", "ã“ã®", "ãŸã‚", "ãã®", "ã‚ˆã†", "ã¾ãŸ", "ã‚‚ã®",
    "ã¾ã™", "ã§ã™", "ã•ã‚“", "ã¡ã‚ƒã‚“", "ãã‚“", "ã‚ã£", "ã‚ã‚Š", "ã„ã£", "ã†",
    "ã‹", "ã›ã‚‹", "ãŸã„", "ã ã‘", "ãŸã¡", "ã¤ã„ã¦", "ã§ã", "ãªã‚Š", "ã®",
    "ã°ã‹ã‚Š", "ã»ã©", "ã¾ã§", "ã¾ã¾", "ã‚ˆã†", "ã‚ˆã‚Š", "ã‚ãŸã—", "ãã‚Œ", "ã“ã‚Œ",
    "å›ç­”", "ãªã—", "ç‰¹ã«ãªã—", "ç‰¹ã«", "ãŸã‚", "ã¦ã", "ãã‚Œã‚‰"
]

# --- 2. é–¢æ•°å®šç¾© ---

def classify_columns(df):
    """å±æ€§(ãƒ•ã‚£ãƒ«ã‚¿ç”¨)ã¨ãƒ†ã‚­ã‚¹ãƒˆ(åˆ†æç”¨)ã‚’è‡ªå‹•åˆ¤å®š"""
    filter_cols = [] 
    text_cols = []   
    for col in df.columns:
        unique_count = df[col].nunique()
        if unique_count < 50:
            filter_cols.append(col)
        elif df[col].dtype == 'object':
            text_cols.append(col)
    return filter_cols, text_cols

@st.cache_data
def get_tokens(text, stop_words):
    """å½¢æ…‹ç´ è§£æ (é™¤å¤–ãƒ¯ãƒ¼ãƒ‰é©ç”¨)"""
    t = Tokenizer()
    tokens = []
    if not isinstance(text, str):
        return []
    
    # é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚»ãƒƒãƒˆï¼ˆæ¤œç´¢é«˜é€ŸåŒ–ï¼‰
    stop_set = set(stop_words)
    japanese_pattern = re.compile(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]')
    
    for token in t.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(',')[0]
        # é™¤å¤–åˆ¤å®š
        if (pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and 
            len(base) > 1 and 
            base not in stop_set and 
            japanese_pattern.search(base)):
            tokens.append(base)
    return tokens

@st.cache_data
def create_network(tokens_list, top_n, min_edge):
    """é€šå¸¸ã®ã‚¨ãƒƒã‚¸ç”Ÿæˆ"""
    pair_list = []
    for tokens in tokens_list:
        if len(tokens) >= 2:
            pair_list.extend(itertools.combinations(tokens, 2))
    c = Counter(pair_list)
    top_pairs = c.most_common(top_n)
    G = nx.Graph()
    for (u, v), weight in top_pairs:
        if weight >= min_edge:
            G.add_edge(u, v, weight=weight)
    return G

def create_colored_network(tokens_a, tokens_b, top_n, min_edge):
    """æ¯”è¼ƒç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (è‰²åˆ†ã‘æ©Ÿèƒ½ä»˜ã)"""
    # ä¸¡æ–¹ã®å˜èªã‚«ã‚¦ãƒ³ãƒˆ
    count_a = Counter(tokens_a)
    count_b = Counter(tokens_b)
    
    # çµåˆã—ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½œã‚‹
    all_tokens_list = [tokens_a, tokens_b] # ç°¡æ˜“çš„ã«2æ–‡æ›¸ã¨ã—ã¦æ‰±ã†ã¨ã‚¨ãƒƒã‚¸ãŒå¼±ããªã‚‹ãŸã‚å·¥å¤«ãŒå¿…è¦
    # å®Ÿéš›ã«ã¯æ–‡æ›¸ã”ã¨ã®ãƒªã‚¹ãƒˆãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯ç°¡æ˜“åŒ–ã®ãŸã‚ã€Œé »å‡ºèªãƒªã‚¹ãƒˆã€ã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚’ä½œã‚‹
    
    # ä¸Šä½èªã‚’æŠ½å‡º
    common_words = set([w for w, c in count_a.most_common(top_n)] + [w for w, c in count_b.most_common(top_n)])
    
    # ã‚¨ãƒƒã‚¸ã®ç”Ÿæˆï¼ˆå…±èµ·ï¼‰ã¯ã€Œå…ƒã®æ–‡è„ˆã€ãŒå¿…è¦ã ãŒã€
    # ã“ã“ã§ã¯è¨ˆç®—è² è·ã‚’ä¸‹ã’ã‚‹ãŸã‚ã€ç°¡æ˜“çš„ã«ãƒãƒ¼ãƒ‰ã®è‰²åˆ†ã‘ã«æ³¨åŠ›ã™ã‚‹
    # ãƒãƒ¼ãƒ‰ã ã‘å®šç¾©ã—ã¦ã€è‰²ã¯ã€Œã©ã¡ã‚‰ã«å¤šãå‡ºã¦ã„ã‚‹ã‹ã€ã§æ±ºã‚ã‚‹
    
    G = nx.Graph()
    
    # ãƒãƒ¼ãƒ‰ã®è¿½åŠ ã¨è‰²æ±ºå®š
    node_colors = []
    for word in common_words:
        freq_a = count_a.get(word, 0)
        freq_b = count_b.get(word, 0)
        total = freq_a + freq_b
        
        if total == 0: continue
        
        G.add_node(word, size=total)
        
        # è‰²åˆ†ã‘ãƒ­ã‚¸ãƒƒã‚¯
        ratio = freq_a / total
        if ratio > 0.7:
            color = "#66b3ff" # é’ (Aå¯„ã‚Š)
        elif ratio < 0.3:
            color = "#ff9999" # èµ¤ (Bå¯„ã‚Š)
        else:
            color = "#dddddd" # ã‚°ãƒ¬ãƒ¼ (å…±é€š)
        
        # å±æ€§ã¨ã—ã¦ä¿å­˜ (æç”»æ™‚ã«ãƒªã‚¹ãƒˆåŒ–ã™ã‚‹ãŸã‚)
        G.nodes[word]['color'] = color
        
    return G

# --- 3. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---

# === ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒªã‚¢ ===
if st.session_state.df is None:
    st.title("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿")
    st.markdown("åˆ†æã—ãŸã„ **CSV** ã¾ãŸã¯ **Excel** ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    uploaded_file = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—", type=['csv', 'xlsx'])
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            st.session_state.df = df
            st.rerun()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# === åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ===
else:
    df = st.session_state.df
    filter_candidates, text_candidates = classify_columns(df)
    
    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
    st.sidebar.title("âš™ï¸ è¨­å®šãƒ‘ãƒãƒ«")
    
    # 1. é™¤å¤–ãƒ¯ãƒ¼ãƒ‰è¨­å®š
    with st.sidebar.expander("ğŸš« é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã®è¨­å®š", expanded=True):
        st.caption("åˆ†æã‹ã‚‰å¤–ã—ãŸã„å˜èªã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§å…¥åŠ›")
        new_words_input = st.text_input("è¿½åŠ ", placeholder="ä¾‹: ç§ã€€æ€ã†ã€€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆ")
        
        if new_words_input:
            # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã«å¤‰æ›ã—ã¦åˆ†å‰²
            words = new_words_input.replace('ã€€', ' ').split()
            added_count = 0
            for w in words:
                if w not in st.session_state.user_stopwords:
                    st.session_state.user_stopwords.append(w)
                    added_count += 1
            if added_count > 0:
                st.success(f"{added_count}èªã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                time.sleep(1) # è¿½åŠ ã—ãŸã“ã¨ãŒã‚ã‹ã‚‹ã‚ˆã†ã«å°‘ã—å¾…ã¤
                st.rerun()
        
        # é™¤å¤–ãƒªã‚¹ãƒˆã®è¡¨ç¤ºï¼ˆå‰Šé™¤æ©Ÿèƒ½ä»˜ãã¯è¤‡é›‘ã«ãªã‚‹ã®ã§ã€ãƒªã‚»ãƒƒãƒˆã®ã¿å®Ÿè£…ï¼‰
        st.write(f"**ç¾åœ¨ã®é™¤å¤–ãƒªã‚¹ãƒˆ ({len(st.session_state.user_stopwords)}èª):**")
        st.text(", ".join(st.session_state.user_stopwords))
        
        if st.button("ãƒªã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ"):
            st.session_state.user_stopwords = []
            st.rerun()
            
    stop_words = DEFAULT_STOPWORDS + st.session_state.user_stopwords

    # 2. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ” å…¨ä½“ã®çµã‚Šè¾¼ã¿")
    
    df_filtered = df.copy()
    for col in filter_candidates:
        unique_vals = sorted(df[col].dropna().unique().tolist())
        selected = st.sidebar.multiselect(f"{col}", unique_vals)
        if selected:
            df_filtered = df_filtered[df_filtered[col].isin(selected)]
            
    st.sidebar.write(f"å¯¾è±¡: {len(df_filtered)} / {len(df)} ä»¶")
    
    if st.sidebar.button("åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"):
        st.session_state.df = None
        st.session_state.user_stopwords = []
        st.rerun()

    # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---
    
    # ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿
    mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰", ["å…¨ä½“åˆ†æ", "âš”ï¸ ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒ"], horizontal=True)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã®çµåˆ
    target_cols = text_candidates if text_candidates else df.columns
    
    if len(df_filtered) == 0:
        st.error("ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚çµã‚Šè¾¼ã¿æ¡ä»¶ã‚’è§£é™¤ã—ã¦ãã ã•ã„ã€‚")

    # === A. å…¨ä½“åˆ†æãƒ¢ãƒ¼ãƒ‰ ===
    elif mode == "å…¨ä½“åˆ†æ":
        full_text = ""
        for col in target_cols:
            full_text += " " + " ".join(df_filtered[col].dropna().astype(str).tolist())
        tokens = get_tokens(full_text, stop_words)

        if not tokens:
            st.warning("è¡¨ç¤ºã§ãã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’æ¸›ã‚‰ã™ã‹ã€ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã—ã¦ãã ã•ã„ã€‚")
        else:
            tab1, tab2, tab3 = st.tabs(["â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ ã¤ãªãŒã‚Šãƒãƒƒãƒ—", "ğŸ“ˆ ãƒ©ãƒ³ã‚­ãƒ³ã‚°"])
            
            with tab1:
                try:
                    wc = WordCloud(
                        background_color="white", width=900, height=500,
                        regexp=r"[\w']+", font_path="IPAexGothic.ttf"
                    ).generate(" ".join(tokens))
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis("off")
                    st.pyplot(fig)
                except:
                    st.error("ãƒ•ã‚©ãƒ³ãƒˆèª­è¾¼ã‚¨ãƒ©ãƒ¼")

            with tab2:
                st.info("ğŸ’¡ **ãƒ’ãƒ³ãƒˆ**: é »ç¹ã«ã‚»ãƒƒãƒˆã§ç™»å ´ã™ã‚‹å˜èªåŒå£«ãŒç·šã§çµã°ã‚Œã¦ã„ã¾ã™ã€‚")
                c1, c2 = st.columns(2)
                net_top = c1.slider("è¡¨ç¤ºå˜èªæ•°", 10, 150, 60)
                min_edge = c2.slider("æœ€å°ã®ç·šã®å¤ªã•", 1, 10, 2)
                
                sentences = []
                for i, row in df_filtered.iterrows():
                    row_text = " ".join([str(row[c]) for c in target_cols if pd.notna(row[c])])
                    sentences.append(row_text)

                tokens_list = [get_tokens(s, stop_words) for s in sentences]
                G = create_network(tokens_list, net_top, min_edge)
                
                if G.number_of_nodes() > 0:
                    fig, ax = plt.subplots(figsize=(8, 8))
                    pos = nx.spring_layout(G, k=0.8, seed=42)
                    nx.draw_networkx_nodes(G, pos, node_size=400, node_color='#66b3ff', alpha=0.9, ax=ax)
                    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color='gray', ax=ax)
                    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_size=10, ax=ax)
                    ax.axis('off')
                    st.pyplot(fig)
                else:
                    st.warning("ã¤ãªãŒã‚ŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

            with tab3:
                c = Counter(tokens)
                words, counts = zip(*c.most_common(20))
                fig, ax = plt.subplots(figsize=(8, 6))
                ax.barh(words, counts, color='skyblue')
                ax.invert_yaxis()
                st.pyplot(fig)

    # === B. ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ (V9å¼·åŒ–ç‰ˆ) ===
    elif mode == "âš”ï¸ ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒ":
        st.markdown("#### 2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®é•ã„ã‚’ä¸€ç”»é¢ã§æ¯”è¼ƒ")
        
        if not filter_candidates:
            st.error("æ¯”è¼ƒã§ãã‚‹å±æ€§åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            col_comp_1, col_comp_2, col_comp_3 = st.columns([1, 1, 1])
            target_attr = col_comp_1.selectbox("æ¯”è¼ƒã™ã‚‹é …ç›®", filter_candidates)
            
            unique_vals = sorted(df_filtered[target_attr].dropna().unique().tolist())
            
            # è¤‡æ•°é¸æŠ (Multiselect) ã«å¤‰æ›´
            vals_a = col_comp_2.multiselect("ã‚°ãƒ«ãƒ¼ãƒ—A (é’)", unique_vals, default=[unique_vals[0]] if unique_vals else None)
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§Aä»¥å¤–ã‚’é¸æŠçŠ¶æ…‹ã«ã™ã‚‹å·¥å¤«
            default_b = [v for v in unique_vals if v not in vals_a]
            if not default_b and unique_vals: default_b = [unique_vals[-1]]
            
            vals_b = col_comp_3.multiselect("ã‚°ãƒ«ãƒ¼ãƒ—B (èµ¤)", unique_vals, default=default_b)

            if not vals_a or not vals_b:
                st.warning("æ¯”è¼ƒã™ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            else:
                # ãƒ‡ãƒ¼ã‚¿åˆ†å‰² & ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                df_a = df_filtered[df_filtered[target_attr].isin(vals_a)]
                df_b = df_filtered[df_filtered[target_attr].isin(vals_b)]
                
                def get_combined_tokens(d):
                    txt = ""
                    for c in target_cols:
                        txt += " " + " ".join(d[c].dropna().astype(str).tolist())
                    return get_tokens(txt, stop_words)

                tokens_a = get_combined_tokens(df_a)
                tokens_b = get_combined_tokens(df_b)

                st.markdown(f"**åˆ†æå¯¾è±¡æ•°:** ğŸŸ¦ ã‚°ãƒ«ãƒ¼ãƒ—A: {len(df_a)}ä»¶ vs ğŸŸ¥ ã‚°ãƒ«ãƒ¼ãƒ—B: {len(df_b)}ä»¶")

                # ã‚¿ãƒ–ã§ã‚°ãƒ©ãƒ•ã‚’åˆ‡ã‚Šæ›¿ãˆ
                comp_tab1, comp_tab2, comp_tab3 = st.tabs(["â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ é•ã„ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ¦‹ å¯¾æ¯”ãƒ©ãƒ³ã‚­ãƒ³ã‚°"])

                with comp_tab1:
                    c1, c2 = st.columns(2)
                    with c1:
                        st.info("ğŸŸ¦ ã‚°ãƒ«ãƒ¼ãƒ—A ã®ç‰¹å¾´")
                        if tokens_a:
                            wc_a = WordCloud(background_color="white", width=400, height=300, font_path="IPAexGothic.ttf").generate(" ".join(tokens_a))
                            fig, ax = plt.subplots()
                            ax.imshow(wc_a, interpolation='bilinear')
                            ax.axis("off")
                            st.pyplot(fig)
                    with c2:
                        st.error("ğŸŸ¥ ã‚°ãƒ«ãƒ¼ãƒ—B ã®ç‰¹å¾´")
                        if tokens_b:
                            wc_b = WordCloud(background_color="white", width=400, height=300, font_path="IPAexGothic.ttf").generate(" ".join(tokens_b))
                            fig, ax = plt.subplots()
                            ax.imshow(wc_b, interpolation='bilinear')
                            ax.axis("off")
                            st.pyplot(fig)

                with comp_tab2:
                    st.markdown("##### ğŸŸ¦ é’ã¯Aã«ã‚ˆãå‡ºã‚‹è¨€è‘‰ã€ğŸŸ¥ èµ¤ã¯Bã«ã‚ˆãå‡ºã‚‹è¨€è‘‰")
                    # ç°¡æ˜“çš„ã«çµåˆã—ãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æç”»ã—ã€ãƒãƒ¼ãƒ‰ã®è‰²ã‚’å¤‰ãˆã‚‹
                    
                    # å…±èµ·è¨ˆç®—ç”¨ã«ä¸€æ—¦sentencesã‚’ä½œã‚‹
                    sentences_mixed = []
                    # Aã®æ–‡
                    for i, row in df_a.iterrows():
                        sentences_mixed.append(" ".join([str(row[c]) for c in target_cols if pd.notna(row[c])]))
                    # Bã®æ–‡
                    for i, row in df_b.iterrows():
                        sentences_mixed.append(" ".join([str(row[c]) for c in target_cols if pd.notna(row[c])]))
                    
                    tokens_list_mixed = [get_tokens(s, stop_words) for s in sentences_mixed]
                    
                    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆ
                    G = create_network(tokens_list_mixed, top_n=60, min_edge=2)
                    
                    if G.number_of_nodes() > 0:
                        # è‰²åˆ†ã‘è¨ˆç®—
                        count_a = Counter(tokens_a)
                        count_b = Counter(tokens_b)
                        
                        node_colors = []
                        for node in G.nodes():
                            fa = count_a.get(node, 0)
                            fb = count_b.get(node, 0)
                            total = fa + fb + 0.1 # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
                            ratio = fa / total
                            
                            if ratio > 0.6:
                                node_colors.append('#66b3ff') # Aå¯„ã‚Š(é’)
                            elif ratio < 0.4:
                                node_colors.append('#ff9999') # Bå¯„ã‚Š(èµ¤)
                            else:
                                node_colors.append('#dddddd') # å…±é€š(ã‚°ãƒ¬ãƒ¼)
                        
                        fig, ax = plt.subplots(figsize=(9, 9))
                        pos = nx.spring_layout(G, k=0.7, seed=42)
                        nx.draw_networkx_nodes(G, pos, node_size=500, node_color=node_colors, alpha=0.9, ax=ax)
                        nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.4, edge_color='gray', ax=ax)
                        nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_size=11, ax=ax)
                        ax.axis('off')
                        st.pyplot(fig)
                    else:
                        st.warning("å…±é€šã™ã‚‹ã¤ãªãŒã‚ŠãŒå°‘ãªã™ãã¦æç”»ã§ãã¾ã›ã‚“ã€‚")

                with comp_tab3:
                    st.markdown("##### ğŸ¦‹ ãƒã‚¿ãƒ•ãƒ©ã‚¤ãƒãƒ£ãƒ¼ãƒˆ (å·¦å³ã®é »åº¦æ¯”è¼ƒ)")
                    # ä¸¡æ–¹ã®ãƒˆãƒƒãƒ—20å˜èªã‚’å–å¾—ã—ã¦ãƒãƒ¼ã‚¸
                    ca = Counter(tokens_a)
                    cb = Counter(tokens_b)
                    
                    # Aã¨Bã‚ã‚ã›ãŸä¸Šä½å˜èª
                    all_top_words = list(set([w for w, c in ca.most_common(15)] + [w for w, c in cb.most_common(15)]))
                    
                    data = []
                    for w in all_top_words:
                        data.append({'word': w, 'A': ca.get(w, 0), 'B': cb.get(w, 0)})
                    
                    df_comp = pd.DataFrame(data).sort_values('A', ascending=True) # Aã®é †ã§ã‚½ãƒ¼ãƒˆ
                    
                    if not df_comp.empty:
                        fig, ax = plt.subplots(figsize=(10, 8))
                        
                        # Aã¯å·¦ï¼ˆãƒã‚¤ãƒŠã‚¹æ–¹å‘ï¼‰ã«ä¼¸ã°ã™
                        ax.barh(df_comp['word'], -df_comp['A'], color='#66b3ff', label=f"ã‚°ãƒ«ãƒ¼ãƒ—A ({len(df_a)})")
                        # Bã¯å³ï¼ˆãƒ—ãƒ©ã‚¹æ–¹å‘ï¼‰ã«ä¼¸ã°ã™
                        ax.barh(df_comp['word'], df_comp['B'], color='#ff9999', label=f"ã‚°ãƒ«ãƒ¼ãƒ—B ({len(df_b)})")
                        
                        # çœŸã‚“ä¸­ã®ç·š
                        ax.axvline(0, color='black', linewidth=0.8)
                        
                        # ãƒ©ãƒ™ãƒ«ï¼ˆãƒã‚¤ãƒŠã‚¹ã‚’ãƒ—ãƒ©ã‚¹è¡¨è¨˜ã«æˆ»ã™ï¼‰
                        xticks = ax.get_xticks()
                        ax.set_xticklabels([str(abs(int(x))) for x in xticks])
                        
                        ax.legend()
                        st.pyplot(fig)
                    else:
                        st.write("ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
