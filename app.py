import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from janome.tokenizer import Tokenizer
from collections import Counter
from wordcloud import WordCloud
import networkx as nx
import itertools
import re
import time

# --- 1. ã‚¢ãƒ—ãƒªã®è¨­å®š ---
st.set_page_config(page_title="Text Analytics V5", layout="wide")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if 'df' not in st.session_state:
    st.session_state.df = None
if 'user_stopwords' not in st.session_state:
    st.session_state.user_stopwords = []
if 'step' not in st.session_state:
    st.session_state.step = 1

# åŸºæœ¬ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
DEFAULT_STOPWORDS = [
    "ã®", "ã«", "ã¯", "ã‚’", "ãŸ", "ãŒ", "ã§", "ã¦", "ã¨", "ã—", "ã‚Œ", "ã•",
    "ã‚ã‚‹", "ã„ã‚‹", "ã‚‚", "ã™ã‚‹", "ã‹ã‚‰", "ãª", "ã“ã¨", "ã¨ã—ã¦", "ã„", "ã‚„",
    "ã‚Œã‚‹", "ãªã©", "ãªã„", "ã“ã®", "ãŸã‚", "ãã®", "ã‚ˆã†", "ã¾ãŸ", "ã‚‚ã®",
    "ã¾ã™", "ã§ã™", "ã•ã‚“", "ã¡ã‚ƒã‚“", "ãã‚“", "ã‚ã£", "ã‚ã‚Š", "ã„ã£", "ã†",
    "ã‹", "ã›ã‚‹", "ãŸã„", "ã ã‘", "ãŸã¡", "ã¤ã„ã¦", "ã§ã", "ãªã‚Š", "ã®",
    "ã°ã‹ã‚Š", "ã»ã©", "ã¾ã§", "ã¾ã¾", "ã‚ˆã†", "ã‚ˆã‚Š", "ã‚ãŸã—", "ãã‚Œ", "ã“ã‚Œ",
    "å›ç­”", "ãªã—", "ç‰¹ã«ãªã—", "ç‰¹ã«", "ãŸã‚"
]

# --- 2. é–¢æ•°å®šç¾© ---

def classify_columns(df):
    """åˆ—ã®ä¸­èº«ã‚’è¦‹ã¦ã€å±æ€§(ãƒ•ã‚£ãƒ«ã‚¿ç”¨)ã‹ãƒ†ã‚­ã‚¹ãƒˆ(åˆ†æç”¨)ã‹ã‚’è‡ªå‹•åˆ¤å®šã™ã‚‹"""
    filter_cols = [] # å­¦å¹´ã€æ€§åˆ¥ãªã©
    text_cols = []   # è‡ªç”±è¨˜è¿°ãªã©

    for col in df.columns:
        # æ•°å€¤å‹ã§ã‚‚ã€ç¨®é¡ãŒå°‘ãªã‘ã‚Œã°ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆå­¦å¹´ãªã©ï¼‰ã¨ã¿ãªã™
        unique_count = df[col].nunique()
        
        # åˆ¤å®šåŸºæº–: ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ãŒ50ç¨®é¡æœªæº€ãªã‚‰ã€Œå±æ€§ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ç”¨ï¼‰ã€ã¨ã¿ãªã™
        if unique_count < 50:
            filter_cols.append(col)
        # ãã‚Œä»¥å¤–ã§ã€æ–‡å­—å‹ãªã‚‰ã€Œãƒ†ã‚­ã‚¹ãƒˆï¼ˆåˆ†æç”¨ï¼‰ã€ã¨ã¿ãªã™
        elif df[col].dtype == 'object':
            text_cols.append(col)
            
    return filter_cols, text_cols

@st.cache_data
def get_tokens(text, stop_words):
    """å½¢æ…‹ç´ è§£æ"""
    t = Tokenizer()
    tokens = []
    if not isinstance(text, str):
        return []
    
    japanese_pattern = re.compile(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]')
    
    for token in t.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(',')[0]
        if (pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and 
            len(base) > 1 and 
            base not in stop_words and 
            japanese_pattern.search(base)):
            tokens.append(base)
    return tokens

@st.cache_data
def create_network(tokens_list, top_n, min_edge):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆ"""
    pair_list = []
    for tokens in tokens_list:
        if len(tokens) >= 2:
            pair_list.extend(itertools.combinations(tokens, 2))
    
    c = Counter(pair_list)
    top_pairs = c.most_common(top_n)
    
    G = nx.Graph()
    for (u, v), weight in top_pairs:
        if weight >= min_edge:
            G.add_edge(u, v, weight=weight)
    return G

# --- 3. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---

# === STEP 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===
if st.session_state.step == 1:
    st.title("ğŸ“‚ Step 1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿")
    st.info("åˆ†æã—ãŸã„ CSV ã¾ãŸã¯ Excel ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    
    uploaded_file = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—", type=['csv', 'xlsx'])
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            st.session_state.df = df
            st.session_state.step = 2
            st.rerun()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# === STEP 2: é™¤å¤–ãƒ¯ãƒ¼ãƒ‰è¨­å®š (å…¨ãƒ‡ãƒ¼ã‚¿å¯¾è±¡) ===
elif st.session_state.step == 2:
    st.title("ğŸ§¹ Step 2: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°")
    st.markdown("ã“ã“ã§ã¯ãƒ‡ãƒ¼ã‚¿ã®çµã‚Šè¾¼ã¿ã¯è¡Œã‚ãšã€**ãƒ‡ãƒ¼ã‚¿å…¨ä½“**ã«å«ã¾ã‚Œã‚‹ä¸è¦ãªå˜èªï¼ˆé™¤å¤–ãƒ¯ãƒ¼ãƒ‰ï¼‰ã‚’è¨­å®šã—ã¾ã™ã€‚")
    
    df = st.session_state.df
    
    # åˆ—ã®è‡ªå‹•åˆ¤å®š
    filter_candidates, text_candidates = classify_columns(df)
    
    # åˆ†æã™ã‚‹åˆ—ã‚’é¸ã°ã›ã‚‹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆå€™è£œã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠï¼‰
    if text_candidates:
        target_col = st.selectbox("åˆ†æã™ã‚‹æ–‡ç« ã®åˆ—ã‚’é¸ã‚“ã§ãã ã•ã„", text_candidates, index=len(text_candidates)-1)
    else:
        target_col = st.selectbox("åˆ†æã™ã‚‹æ–‡ç« ã®åˆ—ã‚’é¸ã‚“ã§ãã ã•ã„", df.columns) # å€™è£œãŒãªã„å ´åˆã¯å…¨åˆ—ã‹ã‚‰

    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ç¾åœ¨ã®é »å‡ºå˜èª (TOP30)")
        
        # ç¾åœ¨ã®è¨­å®šã§é›†è¨ˆ
        current_stop = DEFAULT_STOPWORDS + st.session_state.user_stopwords
        text_data = " ".join(df[target_col].dropna().astype(str).tolist())
        tokens = get_tokens(text_data, current_stop)
        
        if tokens:
            c = Counter(tokens)
            words, counts = zip(*c.most_common(30))
            
            # ã‚°ãƒ©ãƒ•æç”»
            fig, ax = plt.subplots(figsize=(6, 8))
            ax.barh(words, counts, color='gray')
            ax.invert_yaxis()
            ax.set_title("å…¨ä½“ãƒ©ãƒ³ã‚­ãƒ³ã‚°")
            st.pyplot(fig)
        else:
            st.warning("å˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    with col2:
        st.subheader("é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã®è¿½åŠ ")
        st.info("å·¦ã®ã‚°ãƒ©ãƒ•ã‚’è¦‹ã¦ã€åˆ†æã«ä¸è¦ãªå˜èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        new_word = st.text_input("é™¤å¤–ã—ãŸã„å˜èª (å…¥åŠ›ã—ã¦Enter)", placeholder="ä¾‹: ç§ æ€ã†")
        if new_word:
            words = new_word.split()
            added = []
            for w in words:
                if w not in st.session_state.user_stopwords:
                    st.session_state.user_stopwords.append(w)
                    added.append(w)
            if added:
                st.success(f"é™¤å¤–ã—ã¾ã—ãŸ: {added}")
                time.sleep(0.5)
                st.rerun()
        
        st.write("ğŸš« **ç¾åœ¨ã®é™¤å¤–ãƒªã‚¹ãƒˆ:**")
        st.write(st.session_state.user_stopwords)
        
        if st.button("ãƒªã‚»ãƒƒãƒˆ"):
            st.session_state.user_stopwords = []
            st.rerun()

    st.markdown("---")
    # æ¬¡ã¸é€²ã‚€ã¨ãã«ã€é¸ã‚“ã åˆ—æƒ…å ±ã‚’ä¿å­˜
    if st.button("è¨­å®šå®Œäº†ï¼åˆ†æç”»é¢ã¸é€²ã‚€ (Step 3) >>", type="primary"):
        st.session_state.target_col = target_col
        st.session_state.filter_candidates = filter_candidates # è‡ªå‹•åˆ¤å®šã—ãŸå±æ€§åˆ—ã‚’æ¸¡ã™
        st.session_state.step = 3
        st.rerun()

# === STEP 3: æœ€çµ‚åˆ†æ (å¤šé‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° & å¯è¦–åŒ–) ===
elif st.session_state.step == 3:
    st.title("ğŸ“Š Step 3: è©³ç´°åˆ†æ")
    
    df = st.session_state.df
    target_col = st.session_state.target_col
    filter_candidates = st.session_state.filter_candidates
    stop_words = DEFAULT_STOPWORDS + st.session_state.user_stopwords

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š ---
    st.sidebar.header("ğŸ” ãƒ‡ãƒ¼ã‚¿ã®çµã‚Šè¾¼ã¿")
    st.sidebar.caption("æ¡ä»¶ã‚’æŒ‡å®šã™ã‚‹ã¨ã€ã‚°ãƒ©ãƒ•ãŒè‡ªå‹•ã§æ›´æ–°ã•ã‚Œã¾ã™ã€‚")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†
    df_filtered = df.copy()
    
    # è‡ªå‹•åˆ¤å®šã•ã‚ŒãŸå±æ€§åˆ—ã”ã¨ã«ã€é¸æŠãƒœãƒƒã‚¯ã‚¹ã‚’ä½œã‚‹
    active_filters = []
    for col in filter_candidates:
        unique_vals = sorted(df[col].dropna().unique().tolist())
        selected = st.sidebar.multiselect(f"{col}", unique_vals)
        
        if selected:
            df_filtered = df_filtered[df_filtered[col].isin(selected)]
            active_filters.append(f"{col}:{selected}")
    
    # ãƒ•ã‚£ãƒ«ã‚¿çµæœã®è¡¨ç¤º
    st.sidebar.markdown("---")
    st.sidebar.write(f"**åˆ†æå¯¾è±¡:** {len(df_filtered)} è¡Œ / {len(df)} è¡Œ")
    
    if st.sidebar.button("Step 2 (é™¤å¤–è¨­å®š) ã«æˆ»ã‚‹"):
        st.session_state.step = 2
        st.rerun()
    if st.sidebar.button("Step 1 (ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ) ã«æˆ»ã‚‹"):
        st.session_state.df = None
        st.session_state.step = 1
        st.rerun()

    # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢: å¯è¦–åŒ– ---
    
    # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèª
    if len(df_filtered) == 0:
        st.error("æ¡ä»¶ã«åˆã†ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç·©ã‚ã¦ãã ã•ã„ã€‚")
    else:
        # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
        full_text = " ".join(df_filtered[target_col].dropna().astype(str).tolist())
        tokens = get_tokens(full_text, stop_words)

        if not tokens:
            st.warning("è¡¨ç¤ºã§ãã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            tab1, tab2, tab3 = st.tabs(["â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ“ˆ ãƒ©ãƒ³ã‚­ãƒ³ã‚°"])
            
            with tab1:
                st.subheader("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                try:
                    wc = WordCloud(
                        background_color="white", width=800, height=500,
                        regexp=r"[\w']+", font_path="IPAexGothic.ttf"
                    ).generate(" ".join(tokens))
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis("off")
                    st.pyplot(fig)
                except:
                    st.error("ãƒ•ã‚©ãƒ³ãƒˆèª­è¾¼ã‚¨ãƒ©ãƒ¼")

            with tab2:
                st.subheader("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
                col1, col2 = st.columns(2)
                with col1:
                    net_top = st.slider("ã‚¨ãƒƒã‚¸æ•°", 10, 200, 50, key='net1')
                with col2:
                    min_edge = st.slider("æœ€å°å…±èµ·å›æ•°", 1, 10, 2, key='net2')
                
                # è¡Œã”ã¨ã®ãƒªã‚¹ãƒˆä½œæˆ
                sentences = df_filtered[target_col].dropna().astype(str).tolist()
                tokens_list = [get_tokens(s, stop_words) for s in sentences]
                G = create_network(tokens_list, net_top, min_edge)
                
                if G.number_of_nodes() > 0:
                    fig, ax = plt.subplots(figsize=(10, 10))
                    pos = nx.spring_layout(G, k=0.6, seed=42)
                    nx.draw_networkx_nodes(G, pos, node_size=300, node_color='#66b3ff', alpha=0.9, ax=ax)
                    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color='gray', ax=ax)
                    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_size=11, ax=ax)
                    ax.axis('off')
                    st.pyplot(fig)
                else:
                    st.warning("ã¤ãªãŒã‚ŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è¨­å®šã‚’ç·©ã‚ã¦ãã ã•ã„ã€‚")

            with tab3:
                st.subheader("é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°")
                c = Counter(tokens)
                common = c.most_common(20)
                words, counts = zip(*common)
                fig, ax = plt.subplots(figsize=(8, 6))
                ax.barh(words, counts, color='skyblue')
                ax.invert_yaxis()
                st.pyplot(fig)
