import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from janome.tokenizer import Tokenizer
from collections import Counter
from wordcloud import WordCloud
import re
import time

# --- 1. ã‚¢ãƒ—ãƒªã®åŸºæœ¬è¨­å®š ---
st.set_page_config(
    page_title="Text Analytics App",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®å®šç¾©ï¼ˆãŠå¥½ã¿ã§è¿½åŠ å¯èƒ½ï¼‰
DEFAULT_STOPWORDS = [
    "ã®", "ã«", "ã¯", "ã‚’", "ãŸ", "ãŒ", "ã§", "ã¦", "ã¨", "ã—", "ã‚Œ", "ã•",
    "ã‚ã‚‹", "ã„ã‚‹", "ã‚‚", "ã™ã‚‹", "ã‹ã‚‰", "ãª", "ã“ã¨", "ã¨ã—ã¦", "ã„", "ã‚„",
    "ã‚Œã‚‹", "ãªã©", "ãªã„", "ã“ã®", "ãŸã‚", "ãã®", "ã‚ˆã†", "ã¾ãŸ", "ã‚‚ã®",
    "ã¾ã™", "ã§ã™", "ã•ã‚“", "ã¡ã‚ƒã‚“", "ãã‚“"
]

# --- 2. é–¢æ•°ã®å®šç¾©ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ã£ã¦é«˜é€ŸåŒ–ï¼‰ ---

@st.cache_data
def get_tokens(text, stop_words):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    t = Tokenizer()
    tokens = []
    if not isinstance(text, str):
        return []
    
    for token in t.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(',')[0]
        if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and len(base) > 1 and base not in stop_words:
            tokens.append(base)
    return tokens

@st.cache_data
def create_demo_data():
    """ãƒ‡ãƒ¢ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    data = {
        'å­¦å¹´': ['1å¹´', '1å¹´', '2å¹´', '2å¹´', '3å¹´', '3å¹´', '1å¹´', '2å¹´', '3å¹´', '1å¹´'],
        'æ€§åˆ¥': ['ç”·æ€§', 'å¥³æ€§', 'ç”·æ€§', 'å¥³æ€§', 'ç”·æ€§', 'å¥³æ€§', 'å¥³æ€§', 'ç”·æ€§', 'å¥³æ€§', 'ç”·æ€§'],
        'è‡ªç”±è¨˜è¿°': [
            'é‡æ€§å‘³ã‚ãµã‚Œã‚‹äººæã«ãªã‚ŠãŸã„ã—ã€ä¾¡å€¤å‰µé€ ã‚‚é‡è¦ã ã¨æ€ã†ã€‚',
            'æ–°ã—ã„ä¾¡å€¤ã‚’ä½œã‚‹ãŸã‚ã«ã¯ã€é‡æ€§çš„ãªå‹˜ãŒå¿…è¦ã ã¨æ„Ÿã˜ã‚‹ã€‚',
            'å­¦æ ¡ç”Ÿæ´»ã§é‡æ€§å‘³ã‚’ç£¨ãã€ç¤¾ä¼šã§æ´»èºã—ãŸã„ã€‚',
            'ä¾¡å€¤å‰µé€ äººæã¨ã¯ã€å¤±æ•—ã‚’æã‚Œãšã«æŒ‘æˆ¦ã™ã‚‹äººã®ã“ã¨ã ã€‚',
            'å‹‰å¼·ã ã‘ã§ãªãã€éƒ¨æ´»å‹•ã§ã‚‚é‡æ€§å‘³ã‚’å‡ºã—ã¦ã„ããŸã„ã€‚',
            'å°†æ¥ã¯ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªä»•äº‹ã§ä¾¡å€¤ã‚’ç”Ÿã¿å‡ºã—ãŸã„ã€‚',
            'é‡æ€§å‘³ã¨ã¯ã€å›°é›£ã«ç«‹ã¡å‘ã‹ã†å¼·ã•ã®ã“ã¨ã ã¨æ€ã†ã€‚',
            'ä»²é–“ã¨å”åŠ›ã—ã¦æ–°ã—ã„ä¾¡å€¤ã‚’å‰µé€ ã™ã‚‹ã“ã¨ãŒç›®æ¨™ã§ã™ã€‚',
            'ã‚‚ã£ã¨è‡ªç”±ã«ã€é‡æ€§çš„ã«ç”Ÿãã¦ã„ããŸã„ã€‚',
            'ä¾¡å€¤å‰µé€ ã®ãŸã‚ã«ã¯ã€åŸºç¤çš„ãªçŸ¥è­˜ã‚‚å¤§åˆ‡ã ã€‚'
        ]
    }
    return pd.DataFrame(data)

def generate_wordcloud(text, font_path=None):
    """ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    # Macã‚„Linux(Streamlit Cloud)ç’°å¢ƒã§ã®æ–‡å­—åŒ–ã‘å¯¾ç­–ã¨ã—ã¦ãƒ•ã‚©ãƒ³ãƒˆæŒ‡å®šãŒå¿…è¦ãªå ´åˆãŒã‚ã‚Šã¾ã™
    # ä»Šå›ã¯japanize_matplotlibã®ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã‚’å€Ÿç”¨ã™ã‚‹ã‹ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è©¦ã¿ã¾ã™
    wc = WordCloud(
        background_color="white",
        width=800,
        height=500,
        font_path="IPAexGothic.ttf", # â€»åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
        regexp=r"[\w']+"
    ).generate(text)
    return wc

# --- 3. ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆè¨­å®šãƒ»å…¥åŠ›ã‚¨ãƒªã‚¢ï¼‰ ---
st.sidebar.title("ğŸ›  è¨­å®š & ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")

input_method = st.sidebar.radio("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿æ–¹æ³•", ["ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†", "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURLã‚’å…¥åŠ›"])

df = None

if input_method == "ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†":
    if st.sidebar.button("ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"):
        with st.spinner("ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­..."):
            time.sleep(1) # å‡¦ç†æ„Ÿã®æ¼”å‡º
            df = create_demo_data()
            st.sidebar.success("ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")

else:
    url = st.sidebar.text_input("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URL")
    if st.sidebar.button("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"):
        if url:
            try:
                with st.spinner("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­..."):
                    match = re.search(r'/d/([a-zA-Z0-9-_]+)', url)
                    if match:
                        file_id = match.group(1)
                        csv_url = f'https://docs.google.com/spreadsheets/d/{file_id}/export?format=csv'
                        df = pd.read_csv(csv_url)
                        st.sidebar.success(f"èª­ã¿è¾¼ã¿æˆåŠŸï¼ ({len(df)}è¡Œ)")
                    else:
                        st.sidebar.error("URLã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
            except Exception as e:
                st.sidebar.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                st.sidebar.info("ãƒ’ãƒ³ãƒˆ: ã‚·ãƒ¼ãƒˆã®å…±æœ‰è¨­å®šãŒã€Œãƒªãƒ³ã‚¯ã‚’çŸ¥ã£ã¦ã„ã‚‹å…¨å“¡ã€ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

# --- 4. ãƒ¡ã‚¤ãƒ³ç”»é¢ã®æ§‹ç¯‰ ---
st.title("ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¢ãƒ—ãƒª")
st.markdown("ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãªã©ã®è‡ªç”±è¨˜è¿°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æãƒ»å¯è¦–åŒ–ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚")

if df is not None:
    # ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ã‚¿ãƒ–ã‚’è¡¨ç¤º
    tab1, tab2, tab3 = st.tabs(["ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèª", "ğŸ“ˆ é »å‡ºå˜èªåˆ†æ", "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰"])

    with tab1:
        st.header("èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(df)

    with tab2:
        st.header("é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        
        # åˆ†æã™ã‚‹åˆ—ã‚’é¸æŠ
        text_cols = df.select_dtypes(include=['object']).columns
        target_col = st.selectbox("åˆ†æã™ã‚‹åˆ—ã‚’é¸ã‚“ã§ãã ã•ã„", text_cols, index=len(text_cols)-1)
        
        # è¡¨ç¤ºä»¶æ•°ã®ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼
        top_n = st.slider("è¡¨ç¤ºã™ã‚‹å˜èªæ•°", 5, 50, 10)

        if st.button("ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º"):
            with st.spinner("ãƒ†ã‚­ã‚¹ãƒˆè§£æä¸­..."):
                # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
                text_data = " ".join(df[target_col].dropna().astype(str).tolist())
                tokens = get_tokens(text_data, DEFAULT_STOPWORDS)
                
                if tokens:
                    counter = Counter(tokens)
                    words, counts = zip(*counter.most_common(top_n))
                    
                    # ã‚°ãƒ©ãƒ•æç”»
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.barh(words, counts, color='skyblue')
                    ax.invert_yaxis() # ä¸Šä½ã‚’ä¸Šã«
                    ax.set_title(f"ã€Œ{target_col}ã€ã®é »å‡ºå˜èª TOP{top_n}")
                    st.pyplot(fig)
                else:
                    st.warning("åˆ†æå¯èƒ½ãªå˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    with tab3:
        st.header("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
        target_col_wc = st.selectbox("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã«ã™ã‚‹åˆ—", text_cols, key='wc_select')
        
        if st.button("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ä½œæˆ"):
            with st.spinner("æç”»ä¸­..."):
                text_data = " ".join(df[target_col_wc].dropna().astype(str).tolist())
                tokens = get_tokens(text_data, DEFAULT_STOPWORDS)
                text_space_sep = " ".join(tokens)
                
                try:
                    # ãƒ•ã‚©ãƒ³ãƒˆãƒ‘ã‚¹ã®å•é¡Œã‚’å›é¿ã™ã‚‹ãŸã‚ã®ç°¡æ˜“try-except
                    # Streamlit Cloudç­‰ã§æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒãªã„ã¨æ–‡å­—åŒ–ã‘ã™ã‚‹ãŸã‚ã€
                    # å®Ÿéš›ã«ã¯ãƒªãƒã‚¸ãƒˆãƒªã« IPAexGothic.ttf ãªã©ã‚’ç½®ã„ã¦æŒ‡å®šã™ã‚‹ã®ãŒç¢ºå®Ÿã§ã™
                    wc = WordCloud(
                        background_color="white",
                        width=800, height=500,
                        regexp=r"[\w']+",
                        font_path="IPAexGothic.ttf" # ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å‰æ
                    ).generate(text_space_sep)
                    
                    fig_wc, ax_wc = plt.subplots(figsize=(12, 8))
                    ax_wc.imshow(wc, interpolation='bilinear')
                    ax_wc.axis("off")
                    st.pyplot(fig_wc)
                except Exception as e:
                    st.error("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼ˆãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼‰")
                    st.write(e)

else:
    # ãƒ‡ãƒ¼ã‚¿æœªèª­ã¿è¾¼ã¿æ™‚ã®æ¡ˆå†…
    st.info("ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€Œãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã€ã‚’é¸æŠã—ã¦è©¦ã™ã‹ã€ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
