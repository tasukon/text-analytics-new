import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from janome.tokenizer import Tokenizer
from collections import Counter
from wordcloud import WordCloud
import networkx as nx
import itertools
import re
import time

# --- 1. ã‚¢ãƒ—ãƒªã®è¨­å®š ---
st.set_page_config(page_title="Text Analytics V8", layout="wide")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if 'df' not in st.session_state:
    st.session_state.df = None
if 'user_stopwords' not in st.session_state:
    st.session_state.user_stopwords = []

# åŸºæœ¬ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
DEFAULT_STOPWORDS = [
    "ã®", "ã«", "ã¯", "ã‚’", "ãŸ", "ãŒ", "ã§", "ã¦", "ã¨", "ã—", "ã‚Œ", "ã•",
    "ã‚ã‚‹", "ã„ã‚‹", "ã‚‚", "ã™ã‚‹", "ã‹ã‚‰", "ãª", "ã“ã¨", "ã¨ã—ã¦", "ã„", "ã‚„",
    "ã‚Œã‚‹", "ãªã©", "ãªã„", "ã“ã®", "ãŸã‚", "ãã®", "ã‚ˆã†", "ã¾ãŸ", "ã‚‚ã®",
    "ã¾ã™", "ã§ã™", "ã•ã‚“", "ã¡ã‚ƒã‚“", "ãã‚“", "ã‚ã£", "ã‚ã‚Š", "ã„ã£", "ã†",
    "ã‹", "ã›ã‚‹", "ãŸã„", "ã ã‘", "ãŸã¡", "ã¤ã„ã¦", "ã§ã", "ãªã‚Š", "ã®",
    "ã°ã‹ã‚Š", "ã»ã©", "ã¾ã§", "ã¾ã¾", "ã‚ˆã†", "ã‚ˆã‚Š", "ã‚ãŸã—", "ãã‚Œ", "ã“ã‚Œ",
    "å›ç­”", "ãªã—", "ç‰¹ã«ãªã—", "ç‰¹ã«", "ãŸã‚", "ã¦ã", "ãã‚Œã‚‰"
]

# --- 2. é–¢æ•°å®šç¾© ---

def classify_columns(df):
    """å±æ€§(ãƒ•ã‚£ãƒ«ã‚¿ç”¨)ã¨ãƒ†ã‚­ã‚¹ãƒˆ(åˆ†æç”¨)ã‚’è‡ªå‹•åˆ¤å®š"""
    filter_cols = [] 
    text_cols = []   
    for col in df.columns:
        unique_count = df[col].nunique()
        if unique_count < 50:
            filter_cols.append(col)
        elif df[col].dtype == 'object':
            text_cols.append(col)
    return filter_cols, text_cols

@st.cache_data
def get_tokens(text, stop_words):
    """å½¢æ…‹ç´ è§£æ"""
    t = Tokenizer()
    tokens = []
    if not isinstance(text, str):
        return []
    
    japanese_pattern = re.compile(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]')
    
    for token in t.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(',')[0]
        if (pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and 
            len(base) > 1 and 
            base not in stop_words and 
            japanese_pattern.search(base)):
            tokens.append(base)
    return tokens

@st.cache_data
def create_network(tokens_list, top_n, min_edge):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆ"""
    pair_list = []
    for tokens in tokens_list:
        if len(tokens) >= 2:
            pair_list.extend(itertools.combinations(tokens, 2))
    
    c = Counter(pair_list)
    top_pairs = c.most_common(top_n)
    
    G = nx.Graph()
    for (u, v), weight in top_pairs:
        if weight >= min_edge:
            G.add_edge(u, v, weight=weight)
    return G

# --- 3. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---

# === ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒªã‚¢ ===
if st.session_state.df is None:
    st.title("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿")
    st.markdown("åˆ†æã—ãŸã„ **CSV** ã¾ãŸã¯ **Excel** ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    uploaded_file = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—", type=['csv', 'xlsx'])
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            st.session_state.df = df
            st.rerun()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# === åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ===
else:
    df = st.session_state.df
    filter_candidates, text_candidates = classify_columns(df)
    
    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ (é™¤å¤–è¨­å®šï¼†ãƒ•ã‚£ãƒ«ã‚¿) ---
    st.sidebar.title("âš™ï¸ è¨­å®šãƒ‘ãƒãƒ«")
    
    # 1. é™¤å¤–ãƒ¯ãƒ¼ãƒ‰è¨­å®š (Step 2ã®æ©Ÿèƒ½ã‚’ã“ã“ã«çµ±åˆ)
    with st.sidebar.expander("ğŸš« é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã®è¨­å®š", expanded=True):
        st.write(f"ç¾åœ¨: {len(st.session_state.user_stopwords)} èªã‚’é™¤å¤–ä¸­")
        new_word = st.text_input("é™¤å¤–ã—ãŸã„å˜èªã‚’å…¥åŠ›", placeholder="å…¥åŠ›ã—ã¦Enter")
        if new_word:
            words = new_word.split()
            for w in words:
                if w not in st.session_state.user_stopwords:
                    st.session_state.user_stopwords.append(w)
            st.rerun()
        
        if st.button("é™¤å¤–ãƒªã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ"):
            st.session_state.user_stopwords = []
            st.rerun()
            
    stop_words = DEFAULT_STOPWORDS + st.session_state.user_stopwords

    # 2. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ” ãƒ‡ãƒ¼ã‚¿ã®çµã‚Šè¾¼ã¿")
    
    df_filtered = df.copy()
    for col in filter_candidates:
        unique_vals = sorted(df[col].dropna().unique().tolist())
        selected = st.sidebar.multiselect(f"{col}", unique_vals)
        if selected:
            df_filtered = df_filtered[df_filtered[col].isin(selected)]
            
    st.sidebar.write(f"å¯¾è±¡: {len(df_filtered)} / {len(df)} ä»¶")
    
    if st.sidebar.button("åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"):
        st.session_state.df = None
        st.session_state.user_stopwords = []
        st.rerun()

    # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢: åˆ†æçµæœ ---
    
    # ãƒ¢ãƒ¼ãƒ‰åˆ‡æ›¿ã‚¹ã‚¤ãƒƒãƒ
    mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰", ["å…¨ä½“åˆ†æ", "âš”ï¸ ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒ"], horizontal=True)

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã®çµåˆ
    target_cols = text_candidates if text_candidates else df.columns
    
    if len(df_filtered) == 0:
        st.error("ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚çµã‚Šè¾¼ã¿æ¡ä»¶ã‚’è§£é™¤ã—ã¦ãã ã•ã„ã€‚")
        
    # === A. å…¨ä½“åˆ†æãƒ¢ãƒ¼ãƒ‰ ===
    elif mode == "å…¨ä½“åˆ†æ":
        full_text = ""
        for col in target_cols:
            full_text += " " + " ".join(df_filtered[col].dropna().astype(str).tolist())
        tokens = get_tokens(full_text, stop_words)

        if not tokens:
            st.warning("è¡¨ç¤ºã§ãã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            tab1, tab2, tab3 = st.tabs(["â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ ã¤ãªãŒã‚Šãƒãƒƒãƒ—", "ğŸ“ˆ ãƒ©ãƒ³ã‚­ãƒ³ã‚°"])
            
            with tab1:
                st.markdown("#### å…¨ä½“ã®å‚¾å‘ (ç›´æ„Ÿçš„ã«è¦‹ã‚‹)")
                try:
                    wc = WordCloud(
                        background_color="white", width=900, height=500,
                        regexp=r"[\w']+", font_path="IPAexGothic.ttf"
                    ).generate(" ".join(tokens))
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis("off")
                    st.pyplot(fig)
                except:
                    st.error("ãƒ•ã‚©ãƒ³ãƒˆèª­è¾¼ã‚¨ãƒ©ãƒ¼")

            with tab2:
                st.markdown("#### å˜èªã®ã¤ãªãŒã‚Š (å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯)")
                # è¦ªåˆ‡ãªèª¬æ˜ (V7ã®è‰¯ã•ã‚’ç¶™æ‰¿)
                st.info("ğŸ’¡ **è¦‹æ–¹ã®ãƒ’ãƒ³ãƒˆ**: å¤ªã„ç·šã§ã¤ãªãŒã£ã¦ã„ã‚‹å˜èªã¯ã€ã‚»ãƒƒãƒˆã§ä½¿ã‚ã‚Œã¦ã„ã‚‹è¨€è‘‰ã§ã™ã€‚")
                
                c1, c2 = st.columns(2)
                net_top = c1.slider("è¡¨ç¤ºå˜èªæ•°", 10, 150, 50)
                min_edge = c2.slider("æœ€å°ã®ç·šã®å¤ªã•", 1, 10, 2)
                
                sentences = []
                for i, row in df_filtered.iterrows():
                    row_text = " ".join([str(row[c]) for c in target_cols if pd.notna(row[c])])
                    sentences.append(row_text)

                tokens_list = [get_tokens(s, stop_words) for s in sentences]
                G = create_network(tokens_list, net_top, min_edge)
                
                if G.number_of_nodes() > 0:
                    fig, ax = plt.subplots(figsize=(10, 10))
                    pos = nx.spring_layout(G, k=0.6, seed=42)
                    nx.draw_networkx_nodes(G, pos, node_size=300, node_color='#66b3ff', alpha=0.9, ax=ax)
                    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color='gray', ax=ax)
                    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_size=11, ax=ax)
                    ax.axis('off')
                    st.pyplot(fig)
                else:
                    st.warning("ã¤ãªãŒã‚ŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

            with tab3:
                st.markdown("#### é »å‡ºèªãƒ©ãƒ³ã‚­ãƒ³ã‚°")
                c = Counter(tokens)
                words, counts = zip(*c.most_common(20))
                fig, ax = plt.subplots(figsize=(8, 6))
                ax.barh(words, counts, color='skyblue')
                ax.invert_yaxis()
                st.pyplot(fig)

    # === B. ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ ===
    elif mode == "âš”ï¸ ã‚°ãƒ«ãƒ¼ãƒ—æ¯”è¼ƒ":
        st.markdown("#### 2ã¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã®é•ã„ã‚’è¦‹æ¯”ã¹ã‚‹")
        
        # æ¯”è¼ƒã®è¨­å®š
        if not filter_candidates:
            st.error("æ¯”è¼ƒã§ãã‚‹å±æ€§åˆ—ï¼ˆã‚¯ãƒ©ã‚¹ã‚„æ€§åˆ¥ãªã©ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            col_comp_1, col_comp_2, col_comp_3 = st.columns(3)
            target_attr = col_comp_1.selectbox("ã©ã®é …ç›®ã§åˆ†ã‘ã¾ã™ã‹ï¼Ÿ", filter_candidates)
            
            unique_vals = sorted(df_filtered[target_attr].dropna().unique().tolist())
            if len(unique_vals) < 2:
                st.warning("æ¯”è¼ƒã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒè¶³ã‚Šã¾ã›ã‚“ï¼ˆ1ç¨®é¡ã—ã‹ã‚ã‚Šã¾ã›ã‚“ï¼‰ã€‚")
            else:
                val_a = col_comp_2.selectbox("ã‚°ãƒ«ãƒ¼ãƒ—A (å·¦)", unique_vals, index=0)
                val_b = col_comp_3.selectbox("ã‚°ãƒ«ãƒ¼ãƒ—B (å³)", unique_vals, index=min(1, len(unique_vals)-1))

                # ãƒ‡ãƒ¼ã‚¿åˆ†å‰² & ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                df_a = df_filtered[df_filtered[target_attr] == val_a]
                df_b = df_filtered[df_filtered[target_attr] == val_b]
                
                def get_text_tokens(d):
                    txt = ""
                    for c in target_cols:
                        txt += " " + " ".join(d[c].dropna().astype(str).tolist())
                    return get_tokens(txt, stop_words)

                tokens_a = get_text_tokens(df_a)
                tokens_b = get_text_tokens(df_b)

                # å·¦å³ã«ä¸¦ã¹ã¦è¡¨ç¤º
                c_left, c_right = st.columns(2)
                
                with c_left:
                    st.info(f"ğŸŸ¦ {val_a} ({len(df_a)}ä»¶)")
                    if tokens_a:
                        wc_a = WordCloud(background_color="white", width=400, height=300, font_path="IPAexGothic.ttf").generate(" ".join(tokens_a))
                        fig_a, ax_a = plt.subplots()
                        ax_a.imshow(wc_a, interpolation='bilinear')
                        ax_a.axis("off")
                        st.pyplot(fig_a)
                    else:
                        st.write("ãƒ‡ãƒ¼ã‚¿ãªã—")

                with c_right:
                    st.success(f"ğŸŸ§ {val_b} ({len(df_b)}ä»¶)")
                    if tokens_b:
                        wc_b = WordCloud(background_color="white", width=400, height=300, font_path="IPAexGothic.ttf").generate(" ".join(tokens_b))
                        fig_b, ax_b = plt.subplots()
                        ax_b.imshow(wc_b, interpolation='bilinear')
                        ax_b.axis("off")
                        st.pyplot(fig_b)
                    else:
                        st.write("ãƒ‡ãƒ¼ã‚¿ãªã—")
