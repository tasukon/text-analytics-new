import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from janome.tokenizer import Tokenizer
from collections import Counter
from wordcloud import WordCloud
import networkx as nx
import itertools
import re
import time

# --- 1. ã‚¢ãƒ—ãƒªã®è¨­å®š ---
st.set_page_config(page_title="Text Analytics V13", layout="wide")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if 'df' not in st.session_state:
    st.session_state.df = None
if 'user_stopwords' not in st.session_state:
    st.session_state.user_stopwords = []

# åŸºæœ¬ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
DEFAULT_STOPWORDS = [
    "ã®", "ã«", "ã¯", "ã‚’", "ãŸ", "ãŒ", "ã§", "ã¦", "ã¨", "ã—", "ã‚Œ", "ã•",
    "ã‚ã‚‹", "ã„ã‚‹", "ã‚‚", "ã™ã‚‹", "ã‹ã‚‰", "ãª", "ã“ã¨", "ã¨ã—ã¦", "ã„", "ã‚„",
    "ã‚Œã‚‹", "ãªã©", "ãªã„", "ã“ã®", "ãŸã‚", "ãã®", "ã‚ˆã†", "ã¾ãŸ", "ã‚‚ã®",
    "ã¾ã™", "ã§ã™", "ã•ã‚“", "ã¡ã‚ƒã‚“", "ãã‚“", "ã‚ã£", "ã‚ã‚Š", "ã„ã£", "ã†",
    "ã‹", "ã›ã‚‹", "ãŸã„", "ã ã‘", "ãŸã¡", "ã¤ã„ã¦", "ã§ã", "ãªã‚Š", "ã®",
    "ã°ã‹ã‚Š", "ã»ã©", "ã¾ã§", "ã¾ã¾", "ã‚ˆã†", "ã‚ˆã‚Š", "ã‚ãŸã—", "ãã‚Œ", "ã“ã‚Œ",
    "å›ç­”", "ãªã—", "ç‰¹ã«ãªã—", "ç‰¹ã«", "ãŸã‚", "ã¦ã", "ãã‚Œã‚‰"
]

# --- 2. é–¢æ•°å®šç¾© ---

def classify_columns(df):
    """å±æ€§(ãƒ•ã‚£ãƒ«ã‚¿ç”¨)ã¨ãƒ†ã‚­ã‚¹ãƒˆ(åˆ†æç”¨)ã‚’è‡ªå‹•åˆ¤å®š"""
    filter_cols = [] 
    text_cols = []   
    for col in df.columns:
        unique_count = df[col].nunique()
        if unique_count < 50:
            filter_cols.append(col)
        elif df[col].dtype == 'object':
            text_cols.append(col)
    return filter_cols, text_cols

@st.cache_data
def get_tokens(text, stop_words):
    """å½¢æ…‹ç´ è§£æ (é™¤å¤–ãƒ¯ãƒ¼ãƒ‰é©ç”¨)"""
    t = Tokenizer()
    tokens = []
    if not isinstance(text, str):
        return []
    
    stop_set = set(stop_words)
    japanese_pattern = re.compile(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]')
    
    for token in t.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(',')[0]
        if (pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and 
            len(base) > 1 and 
            base not in stop_set and 
            japanese_pattern.search(base)):
            tokens.append(base)
    return tokens

@st.cache_data
def create_network(tokens_list, top_n, min_edge):
    """é€šå¸¸ã®ã‚¨ãƒƒã‚¸ç”Ÿæˆ"""
    pair_list = []
    for tokens in tokens_list:
        if len(tokens) >= 2:
            pair_list.extend(itertools.combinations(tokens, 2))
    c = Counter(pair_list)
    top_pairs = c.most_common(top_n)
    G = nx.Graph()
    for (u, v), weight in top_pairs:
        if weight >= min_edge:
            G.add_edge(u, v, weight=weight)
    return G

def display_kwic(df_target, target_cols, search_words_list, filter_cols):
    """åŸæ–‡æ¤œç´¢çµæœã‚’è¡¨ç¤º (è¤‡æ•°å˜èªANDæ¤œç´¢ãƒ»å±æ€§ã‚¿ã‚°ä»˜ã)"""
    count = 0
    
    for i, row in df_target.iterrows():
        row_text = " ".join([str(row[c]) for c in target_cols if pd.notna(row[c])])
        
        # ANDæ¤œç´¢: ãƒªã‚¹ãƒˆå†…ã®å˜èªãŒã€Œã™ã¹ã¦ã€å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
        if all(word in row_text for word in search_words_list):
            count += 1
            
            # ãƒ’ãƒƒãƒˆã—ãŸå˜èªã™ã¹ã¦ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
            highlighted_text = row_text
            for word in search_words_list:
                highlighted_text = highlighted_text.replace(word, f"**{word}**")
            
            # å±æ€§ã‚¿ã‚°ã®ä½œæˆ
            tags = []
            for f_col in filter_cols:
                val = row[f_col]
                if pd.notna(val):
                    tags.append(f"[{val}]")
            tag_str = " ".join(tags)
            
            # è¡¨ç¤º
            st.markdown(f"ğŸ·ï¸ **{tag_str}** : {highlighted_text}")
            st.markdown("---")
            
            if count >= 20:
                st.caption(f"â€»ã“ã‚Œä»¥ä¸Šã¯çœç•¥ã—ã¾ã™ï¼ˆä»– {len(df_target)-count} ä»¶ã®å¯èƒ½æ€§ã‚ã‚Šï¼‰")
                break
    
    if count == 0:
        st.write("æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹æ–‡ç« ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

# --- 3. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---

# === ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒªã‚¢ ===
if st.session_state.df is None:
    st.title("ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿")
    st.markdown("åˆ†æã—ãŸã„ **CSV** ã¾ãŸã¯ **Excel** ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    uploaded_file = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—", type=['csv', 'xlsx'])
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            st.session_state.df = df
            st.rerun()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# === åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ ===
else:
    df = st.session_state.df
    filter_candidates, text_candidates = classify_columns(df)
    
    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: è¨­å®šã‚¨ãƒªã‚¢ ---
    st.sidebar.title("âš™ï¸ è¨­å®šãƒ‘ãƒãƒ«")
    
    # 1. é™¤å¤–ãƒ¯ãƒ¼ãƒ‰è¨­å®š
    with st.sidebar.expander("ğŸš« é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã®è¨­å®š", expanded=True):
        st.caption("åˆ†æã‹ã‚‰å¤–ã—ãŸã„å˜èªã‚’ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§å…¥åŠ›")
        new_words_input = st.text_input("è¿½åŠ ", placeholder="ä¾‹: ç§ã€€æ€ã†ã€€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆ")
        
        if new_words_input:
            words = new_words_input.replace('ã€€', ' ').split()
            for w in words:
                if w not in st.session_state.user_stopwords:
                    st.session_state.user_stopwords.append(w)
            st.rerun()
        
        st.write(f"**ç¾åœ¨ã®é™¤å¤–ãƒªã‚¹ãƒˆ ({len(st.session_state.user_stopwords)}èª):**")
        st.text(", ".join(st.session_state.user_stopwords))
        
        if st.button("ãƒªã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ"):
            st.session_state.user_stopwords = []
            st.rerun()
            
    stop_words = DEFAULT_STOPWORDS + st.session_state.user_stopwords

    # 2. å…¨ä½“ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ” å…¨ä½“ã®çµã‚Šè¾¼ã¿")
    
    df_filtered = df.copy()
    for col in filter_candidates:
        unique_vals = sorted(df[col].dropna().unique().tolist())
        selected = st.sidebar.multiselect(f"{col}", unique_vals)
        if selected:
            df_filtered = df_filtered[df_filtered[col].isin(selected)]
            
    st.sidebar.write(f"å¯¾è±¡: {len(df_filtered)} / {len(df)} ä»¶")
    
    if st.sidebar.button("åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"):
        st.session_state.df = None
        st.session_state.user_stopwords = []
        st.rerun()

    # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ ---
    
    mode = st.radio("è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰", ["å…¨ä½“åˆ†æ", "âš”ï¸ è‡ªç”±æ¯”è¼ƒ (ã‚«ã‚¹ã‚¿ãƒ )"], horizontal=True)

    target_cols = text_candidates if text_candidates else df.columns
    
    if len(df_filtered) == 0:
        st.error("ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚çµã‚Šè¾¼ã¿æ¡ä»¶ã‚’è§£é™¤ã—ã¦ãã ã•ã„ã€‚")

    # === A. å…¨ä½“åˆ†æãƒ¢ãƒ¼ãƒ‰ ===
    elif mode == "å…¨ä½“åˆ†æ":
        full_text = ""
        for col in target_cols:
            full_text += " " + " ".join(df_filtered[col].dropna().astype(str).tolist())
        tokens = get_tokens(full_text, stop_words)

        if not tokens:
            st.warning("è¡¨ç¤ºã§ãã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            tab1, tab2, tab3, tab4 = st.tabs(["â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ ã¤ãªãŒã‚Šãƒãƒƒãƒ—", "ğŸ“ˆ ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ğŸ” åŸæ–‡æ¤œç´¢"])
            
            with tab1:
                try:
                    wc = WordCloud(
                        background_color="white", width=900, height=500,
                        regexp=r"[\w']+", font_path="IPAexGothic.ttf"
                    ).generate(" ".join(tokens))
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis("off")
                    st.pyplot(fig)
                except:
                    st.error("ãƒ•ã‚©ãƒ³ãƒˆèª­è¾¼ã‚¨ãƒ©ãƒ¼")

            with tab2:
                st.info("ğŸ’¡ **ãƒ’ãƒ³ãƒˆ**: åŒã˜æ–‡è„ˆã§ã‚ˆãä½¿ã‚ã‚Œã‚‹å˜èªåŒå£«ãŒç·šã§çµã°ã‚Œã¦ã„ã¾ã™ã€‚")
                c1, c2 = st.columns(2)
                net_top = c1.slider("è¡¨ç¤ºå˜èªæ•°", 10, 150, 60)
                min_edge = c2.slider("æœ€å°ã®ç·šã®å¤ªã•", 1, 10, 2)
                
                sentences = []
                for i, row in df_filtered.iterrows():
                    row_text = " ".join([str(row[c]) for c in target_cols if pd.notna(row[c])])
                    sentences.append(row_text)

                tokens_list = [get_tokens(s, stop_words) for s in sentences]
                G = create_network(tokens_list, net_top, min_edge)
                
                if G.number_of_nodes() > 0:
                    fig, ax = plt.subplots(figsize=(8, 8))
                    pos = nx.spring_layout(G, k=0.8, seed=42)
                    nx.draw_networkx_nodes(G, pos, node_size=400, node_color='#66b3ff', alpha=0.9, ax=ax)
                    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color='gray', ax=ax)
                    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_size=10, ax=ax)
                    ax.axis('off')
                    st.pyplot(fig)
                else:
                    st.warning("ã¤ãªãŒã‚ŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

            with tab3:
                c = Counter(tokens)
                words, counts = zip(*c.most_common(20))
                fig, ax = plt.subplots(figsize=(8, 6))
                ax.barh(words, counts, color='skyblue')
                ax.invert_yaxis()
                st.pyplot(fig)

            with tab4:
                st.markdown("#### ğŸ’¬ è¤‡æ•°ã®å˜èªã§æ¤œç´¢ (ANDæ¤œç´¢)")
                st.caption("ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚‹ã¨ã€ãã‚Œã‚‰ã™ã¹ã¦ã‚’å«ã‚€æ–‡ç« ã‚’æ¤œç´¢ã—ã¾ã™ã€‚ä¾‹: ã€Œè‡ªåˆ† ä¾¡å€¤ã€")
                input_str = st.text_input("æ¤œç´¢ã—ãŸã„å˜èª", placeholder="ä¾‹: è‡ªåˆ† ä¾¡å€¤")
                
                if input_str:
                    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã«ã—ã¦åˆ†å‰²
                    search_words = input_str.replace('ã€€', ' ').split()
                    st.markdown(f"**ã€Œ{' + '.join(search_words)}ã€ã‚’å«ã‚€å›ç­”ä¸€è¦§:**")
                    st.markdown("---")
                    display_kwic(df_filtered, target_cols, search_words, filter_candidates)

    # === B. è‡ªç”±æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ ===
    elif mode == "âš”ï¸ è‡ªç”±æ¯”è¼ƒ (ã‚«ã‚¹ã‚¿ãƒ )":
        st.markdown("#### æ¡ä»¶ã‚’çµ„ã¿åˆã‚ã›ã¦ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ")
        
        if not filter_candidates:
            st.error("æ¯”è¼ƒã§ãã‚‹å±æ€§åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            col_a_setup, col_b_setup = st.columns(2)
            
            with col_a_setup:
                st.info("ğŸŸ¦ **ã‚°ãƒ«ãƒ¼ãƒ—A ã®æ¡ä»¶**")
                df_a = df_filtered.copy()
                with st.expander("æ¡ä»¶ã‚’é¸æŠ", expanded=True):
                    for col in filter_candidates:
                        vals = sorted(df[col].dropna().unique().tolist())
                        selected_a = st.multiselect(f"{col} (A)", vals, key=f"sel_a_{col}")
                        if selected_a:
                            df_a = df_a[df_a[col].isin(selected_a)]
                st.write(f"**äººæ•°:** {len(df_a)} äºº")

            with col_b_setup:
                st.error("ğŸŸ¥ **ã‚°ãƒ«ãƒ¼ãƒ—B ã®æ¡ä»¶**")
                df_b = df_filtered.copy()
                with st.expander("æ¡ä»¶ã‚’é¸æŠ", expanded=True):
                    for col in filter_candidates:
                        vals = sorted(df[col].dropna().unique().tolist())
                        selected_b = st.multiselect(f"{col} (B)", vals, key=f"sel_b_{col}")
                        if selected_b:
                            df_b = df_b[df_b[col].isin(selected_b)]
                st.write(f"**äººæ•°:** {len(df_b)} äºº")

            if len(df_a) == 0 or len(df_b) == 0:
                st.warning("æ¡ä»¶ã«è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚")
            else:
                def get_combined_tokens(d):
                    txt = ""
                    for c in target_cols:
                        txt += " " + " ".join(d[c].dropna().astype(str).tolist())
                    return get_tokens(txt, stop_words)

                tokens_a = get_combined_tokens(df_a)
                tokens_b = get_combined_tokens(df_b)

                comp_tab1, comp_tab2, comp_tab3, comp_tab4 = st.tabs(["â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ é•ã„ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ¦‹ å¯¾æ¯”ãƒ©ãƒ³ã‚­ãƒ³ã‚°", "ğŸ” åŸæ–‡æ¤œç´¢"])

                with comp_tab1:
                    c1, c2 = st.columns(2)
                    with c1:
                        if tokens_a:
                            wc_a = WordCloud(background_color="white", width=400, height=300, font_path="IPAexGothic.ttf").generate(" ".join(tokens_a))
                            fig, ax = plt.subplots()
                            ax.imshow(wc_a, interpolation='bilinear')
                            ax.axis("off")
                            st.pyplot(fig)
                    with c2:
                        if tokens_b:
                            wc_b = WordCloud(background_color="white", width=400, height=300, font_path="IPAexGothic.ttf").generate(" ".join(tokens_b))
                            fig, ax = plt.subplots()
                            ax.imshow(wc_b, interpolation='bilinear')
                            ax.axis("off")
                            st.pyplot(fig)

                with comp_tab2:
                    st.markdown("##### ğŸŸ¦ é’ã¯Aã®ç‰¹å¾´ã€ğŸŸ¥ èµ¤ã¯Bã®ç‰¹å¾´ã€â¬œ ã‚°ãƒ¬ãƒ¼ã¯å…±é€š")
                    sentences_mixed = []
                    for i, row in df_a.iterrows():
                        sentences_mixed.append(" ".join([str(row[c]) for c in target_cols if pd.notna(row[c])]))
                    for i, row in df_b.iterrows():
                        sentences_mixed.append(" ".join([str(row[c]) for c in target_cols if pd.notna(row[c])]))
                    
                    tokens_list_mixed = [get_tokens(s, stop_words) for s in sentences_mixed]
                    G = create_network(tokens_list_mixed, top_n=60, min_edge=2)
                    
                    if G.number_of_nodes() > 0:
                        count_a = Counter(tokens_a)
                        count_b = Counter(tokens_b)
                        node_colors = []
                        for node in G.nodes():
                            fa = count_a.get(node, 0)
                            fb = count_b.get(node, 0)
                            total = fa + fb + 0.1 
                            ratio = fa / total
                            if ratio > 0.6: node_colors.append('#66b3ff')
                            elif ratio < 0.4: node_colors.append('#ff9999')
                            else: node_colors.append('#dddddd')
                        
                        fig, ax = plt.subplots(figsize=(9, 9))
                        pos = nx.spring_layout(G, k=0.7, seed=42)
                        nx.draw_networkx_nodes(G, pos, node_size=500, node_color=node_colors, alpha=0.9, ax=ax)
                        nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.4, edge_color='gray', ax=ax)
                        nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_size=11, ax=ax)
                        ax.axis('off')
                        st.pyplot(fig)
                    else:
                        st.warning("å…±é€šãƒ‡ãƒ¼ã‚¿ä¸è¶³")

                with comp_tab3:
                    st.markdown("##### ğŸ¦‹ ãƒã‚¿ãƒ•ãƒ©ã‚¤ãƒãƒ£ãƒ¼ãƒˆ")
                    ca = Counter(tokens_a)
                    cb = Counter(tokens_b)
                    all_top_words = list(set([w for w, c in ca.most_common(15)] + [w for w, c in cb.most_common(15)]))
                    data = []
                    for w in all_top_words:
                        data.append({'word': w, 'A': ca.get(w, 0), 'B': cb.get(w, 0)})
                    df_comp = pd.DataFrame(data).sort_values('A', ascending=True)
                    if not df_comp.empty:
                        fig, ax = plt.subplots(figsize=(10, 8))
                        ax.barh(df_comp['word'], -df_comp['A'], color='#66b3ff', label="ã‚°ãƒ«ãƒ¼ãƒ—A")
                        ax.barh(df_comp['word'], df_comp['B'], color='#ff9999', label="ã‚°ãƒ«ãƒ¼ãƒ—B")
                        ax.axvline(0, color='black', linewidth=0.8)
                        xticks = ax.get_xticks()
                        ax.set_xticklabels([str(abs(int(x))) for x in xticks])
                        ax.legend()
                        st.pyplot(fig)

                with comp_tab4:
                    st.markdown("#### ğŸ’¬ æ–‡è„ˆã®é•ã„ã‚’ç¢ºèªã™ã‚‹ (è¤‡æ•°å˜èªOK)")
                    input_str = st.text_input("æ¤œç´¢ã—ãŸã„å˜èª (ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Š)", placeholder="ä¾‹: æˆæ¥­ æ¥½ã—ã„")
                    
                    if input_str:
                        search_words = input_str.replace('ã€€', ' ').split()
                        col_res_a, col_res_b = st.columns(2)
                        with col_res_a:
                            st.info(f"ğŸŸ¦ ã‚°ãƒ«ãƒ¼ãƒ—Aã®æ¤œç´¢çµæœ")
                            display_kwic(df_a, target_cols, search_words, filter_candidates)
                        with col_res_b:
                            st.error(f"ğŸŸ¥ ã‚°ãƒ«ãƒ¼ãƒ—Bã®æ¤œç´¢çµæœ")
                            display_kwic(df_b, target_cols, search_words, filter_candidates)
