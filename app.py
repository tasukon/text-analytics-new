import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from janome.tokenizer import Tokenizer
from collections import Counter
from wordcloud import WordCloud
import networkx as nx
import itertools
import re # Ê≠£Ë¶èË°®Áèæ„Çí‰Ωø„ÅÜ„Åü„ÇÅ„ÅÆ„É©„Ç§„Éñ„É©„É™

# --- 1. „Ç¢„Éó„É™„ÅÆÂü∫Êú¨Ë®≠ÂÆö ---
st.set_page_config(
    page_title="Text Analytics App",
    page_icon="üìä",
    layout="wide"
)

# „Çª„ÉÉ„Ç∑„Éß„É≥„Çπ„ÉÜ„Éº„ÉàÂàùÊúüÂåñ
if 'df' not in st.session_state:
    st.session_state.df = None

# „Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„ÉâÔºàÊÑèÂë≥„ÅÆ„Å™„ÅÑÂçòË™û„É™„Çπ„ÉàÔºâ
DEFAULT_STOPWORDS = [
    "„ÅÆ", "„Å´", "„ÅØ", "„Çí", "„Åü", "„Åå", "„Åß", "„Å¶", "„Å®", "„Åó", "„Çå", "„Åï",
    "„ÅÇ„Çã", "„ÅÑ„Çã", "„ÇÇ", "„Åô„Çã", "„Åã„Çâ", "„Å™", "„Åì„Å®", "„Å®„Åó„Å¶", "„ÅÑ", "„ÇÑ",
    "„Çå„Çã", "„Å™„Å©", "„Å™„ÅÑ", "„Åì„ÅÆ", "„Åü„ÇÅ", "„Åù„ÅÆ", "„Çà„ÅÜ", "„Åæ„Åü", "„ÇÇ„ÅÆ",
    "„Åæ„Åô", "„Åß„Åô", "„Åï„Çì", "„Å°„ÇÉ„Çì", "„Åè„Çì", "„ÅÇ„Å£", "„ÅÇ„Çä", "„ÅÑ„Å£", "„ÅÜ",
    "„Åã", "„Åõ„Çã", "„Åü„ÅÑ", "„Å†„Åë", "„Åü„Å°", "„Å§„ÅÑ„Å¶", "„Åß„Åç", "„Å™„Çä", "„ÅÆ",
    "„Å∞„Åã„Çä", "„Åª„Å©", "„Åæ„Åß", "„Åæ„Åæ", "„Çà„ÅÜ", "„Çà„Çä", "„Çè„Åü„Åó", "„Åù„Çå", "„Åì„Çå"
]

# --- 2. Èñ¢Êï∞ÂÆöÁæ© ---

@st.cache_data
def get_tokens(text, stop_words):
    """„ÉÜ„Ç≠„Çπ„Éà„Åã„ÇâÂêçË©û„ÉªÂãïË©û„ÉªÂΩ¢ÂÆπË©û„ÇíÊäΩÂá∫Ôºà„Ç¥„Éü„Å®„ÇäÊ©üËÉΩ‰ªò„ÅçÔºâ"""
    t = Tokenizer()
    tokens = []
    if not isinstance(text, str):
        return []
    
    # Êó•Êú¨Ë™û„ÅÆÊñáÂ≠óÔºà„Å≤„Çâ„Åå„Å™„ÄÅ„Ç´„Çø„Ç´„Éä„ÄÅÊº¢Â≠óÔºâ„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ„Åô„ÇãÊ≠£Ë¶èË°®Áèæ
    japanese_pattern = re.compile(r'[„ÅÅ-„Çì„Ç°-„É≥‰∏Ä-Èæ•]')

    for token in t.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(',')[0]
        
        # Êù°‰ª∂Ôºö
        # 1. ÂêçË©û„ÉªÂãïË©û„ÉªÂΩ¢ÂÆπË©û„Åß„ÅÇ„Çã
        # 2. 1ÊñáÂ≠ó„Çà„ÇäÈï∑„ÅÑÔºà"„ÅÇ" "„ÅÑ" „Å™„Å©„ÇíÁúÅ„ÅèÔºâ
        # 3. „Çπ„Éà„ÉÉ„Éó„ÉØ„Éº„Éâ„Å´Âê´„Åæ„Çå„Å¶„ÅÑ„Å™„ÅÑ
        # 4. „ÄêËøΩÂä†„ÄëÊó•Êú¨Ë™û„ÅÆÊñáÂ≠ó„ÇíÂê´„Çì„Åß„ÅÑ„ÇãÔºàË®òÂè∑„ÇÑÊï∞Â≠ó„Å†„Åë„ÅÆ„ÇÇ„ÅÆ„ÇíÁúÅ„ÅèÔºâ
        if (pos in ['ÂêçË©û', 'ÂãïË©û', 'ÂΩ¢ÂÆπË©û'] and 
            len(base) > 1 and 
            base not in stop_words and 
            japanese_pattern.search(base)):
            
            tokens.append(base)
    return tokens

@st.cache_data
def create_cooccurrence_network(tokens_list, top_n=50, min_edge_weight=1):
    """ÂÖ±Ëµ∑„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ‰ΩúÊàê"""
    pair_list = []
    for tokens in tokens_list:
        if len(tokens) >= 2:
            pair_list.extend(itertools.combinations(tokens, 2))
    
    c = Counter(pair_list)
    top_pairs = c.most_common(top_n)
    
    G = nx.Graph()
    for (u, v), weight in top_pairs:
        if weight >= min_edge_weight:
            G.add_edge(u, v, weight=weight)
    return G

@st.cache_data
def create_demo_data():
    """„Éá„É¢„Éá„Éº„ÇøÁîüÊàê"""
    data = {
        'Â≠¶Âπ¥': ['1Âπ¥', '1Âπ¥', '2Âπ¥', '2Âπ¥', '3Âπ¥', '3Âπ¥', '1Âπ¥', '2Âπ¥', '3Âπ¥', '1Âπ¥'],
        'Ëá™Áî±Ë®òËø∞': [
            'ÈáéÊÄßÂë≥„ÅÇ„Åµ„Çå„Çã‰∫∫Êùê„Å´„Å™„Çä„Åü„ÅÑ„Åó„ÄÅ‰æ°ÂÄ§ÂâµÈÄ†„ÇÇÈáçË¶Å„Å†„Å®ÊÄù„ÅÜ„ÄÇ(?)', # Ë®òÂè∑Ê∑∑„Åò„Çä„ÉÜ„Çπ„Éà
            'Êñ∞„Åó„ÅÑ‰æ°ÂÄ§„Çí‰Ωú„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅÈáéÊÄßÁöÑ„Å™Âãò„ÅåÂøÖË¶Å„Å†„Å®ÊÑü„Åò„ÇãÔºÅ',
            'Â≠¶Ê†°ÁîüÊ¥ª„ÅßÈáéÊÄßÂë≥„ÇíÁ£®„Åç„ÄÅÁ§æ‰ºö„ÅßÊ¥ªË∫ç„Åó„Åü„ÅÑ„ÄÇ12345', # Êï∞Â≠óÊ∑∑„Åò„Çä„ÉÜ„Çπ„Éà
            '‰æ°ÂÄ§ÂâµÈÄ†‰∫∫Êùê„Å®„ÅØ„ÄÅÂ§±Êïó„ÇíÊÅê„Çå„Åö„Å´ÊåëÊà¶„Åô„Çã‰∫∫„ÅÆ„Åì„Å®„Å†„ÄÇ',
            'ÂãâÂº∑„Å†„Åë„Åß„Å™„Åè„ÄÅÈÉ®Ê¥ªÂãï„Åß„ÇÇÈáéÊÄßÂë≥„ÇíÂá∫„Åó„Å¶„ÅÑ„Åç„Åü„ÅÑ„ÄÇ',
            'Â∞ÜÊù•„ÅØ„ÇØ„É™„Ç®„Ç§„ÉÜ„Ç£„Éñ„Å™‰ªï‰∫ã„Åß‰æ°ÂÄ§„ÇíÁîü„ÅøÂá∫„Åó„Åü„ÅÑ„ÄÇ',
            'ÈáéÊÄßÂë≥„Å®„ÅØ„ÄÅÂõ∞Èõ£„Å´Á´ã„Å°Âêë„Åã„ÅÜÂº∑„Åï„ÅÆ„Åì„Å®„Å†„Å®ÊÄù„ÅÜ„ÄÇ',
            '‰ª≤Èñì„Å®ÂçîÂäõ„Åó„Å¶Êñ∞„Åó„ÅÑ‰æ°ÂÄ§„ÇíÂâµÈÄ†„Åô„Çã„Åì„Å®„ÅåÁõÆÊ®ô„Åß„Åô„ÄÇ',
            '„ÇÇ„Å£„Å®Ëá™Áî±„Å´„ÄÅÈáéÊÄßÁöÑ„Å´Áîü„Åç„Å¶„ÅÑ„Åç„Åü„ÅÑ„ÄÇ',
            '‰æ°ÂÄ§ÂâµÈÄ†„ÅÆ„Åü„ÇÅ„Å´„ÅØ„ÄÅÂü∫Á§éÁöÑ„Å™Áü•Ë≠ò„ÇÇÂ§ßÂàá„Å†„ÄÇ'
        ]
    }
    return pd.DataFrame(data)

# --- 3. ÁîªÈù¢Ë°®Á§∫ ---

if st.session_state.df is None:
    # === Step 1: „Éï„Ç°„Ç§„É´„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÁîªÈù¢ ===
    st.title("üìÇ „ÉÜ„Ç≠„Çπ„ÉàÂàÜÊûê„Ç¢„Éó„É™")
    st.info("‰∏ã„ÅÆ„Éú„ÉÉ„ÇØ„Çπ„Åã„Çâ CSV „Åæ„Åü„ÅØ Excel„Éï„Ç°„Ç§„É´ „ÇíË™≠„ÅøËæº„Çì„Åß„Åè„Å†„Åï„ÅÑ")
    
    uploaded_file = st.file_uploader("„Éï„Ç°„Ç§„É´„Çí„Åì„Åì„Å´„Éâ„É©„ÉÉ„Ç∞ÔºÜ„Éâ„É≠„ÉÉ„Éó", type=['csv', 'xlsx'])

    if uploaded_file is not None:
        try:
            with st.spinner("Ë™≠„ÅøËæº„Åø‰∏≠..."):
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                st.session_state.df = df
                st.rerun()
        except Exception as e:
            st.error(f"„Ç®„É©„Éº: {e}")

    st.markdown("---")
    if st.button("„Åæ„Åö„ÅØ„Éá„É¢„Éá„Éº„Çø„ÅßË©¶„Åô"):
        st.session_state.df = create_demo_data()
        st.rerun()

else:
    # === Step 2: ÂàÜÊûêÁîªÈù¢ ===
    df = st.session_state.df
    
    # „Çµ„Ç§„Éâ„Éê„ÉºË®≠ÂÆö
    with st.sidebar:
        st.header("„É°„Éã„É•„Éº")
        # „É™„Çª„ÉÉ„Éà„Éú„Çø„É≥
        if st.button("üîÑ ÊúÄÂàù„Å´Êàª„Çã"):
            st.session_state.df = None
            st.rerun()
        st.markdown("---")
        
        # ÂàÜÊûêÂØæË±°„ÅÆÈÅ∏Êäû
        st.subheader("ÂàÜÊûêË®≠ÂÆö")
        all_cols = df.columns
        target_col = st.selectbox("ÂàÜÊûê„Åô„ÇãÂàó", all_cols, index=len(all_cols)-1)

    st.title(f"üìä ÂàÜÊûêÁµêÊûú: {target_col}")

    # „Çø„ÉñË®≠ÂÆö
    tab1, tab2, tab3, tab4 = st.tabs(["üìÇ „Éá„Éº„Çø", "üìà „É©„É≥„Ç≠„É≥„Ç∞", "‚òÅÔ∏è „ÉØ„Éº„Éâ„ÇØ„É©„Ç¶„Éâ", "üï∏Ô∏è ÂÖ±Ëµ∑„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ"])

    with tab1:
        st.dataframe(df)

    with tab2:
        st.subheader("È†ªÂá∫ÂçòË™û„É©„É≥„Ç≠„É≥„Ç∞")
        # „Çπ„É©„Ç§„ÉÄ„Éº
        top_n = st.slider("Ë°®Á§∫‰ª∂Êï∞", 5, 100, 20, key='bar_slider')
        
        if st.button("„Ç∞„É©„Éï„ÇíË°®Á§∫", key='btn_bar'):
            with st.spinner("ÈõÜË®à‰∏≠..."):
                text_data = " ".join(df[target_col].dropna().astype(str).tolist())
                tokens = get_tokens(text_data, DEFAULT_STOPWORDS)
                
                if tokens:
                    counter = Counter(tokens)
                    words, counts = zip(*counter.most_common(top_n))
                    
                    # --- „ÄêÊîπÂñÑÁÇπ„Äë„Ç∞„É©„Éï„ÅÆÈ´ò„Åï„ÇíËá™ÂãïË™øÊï¥ ---
                    # 1„Éá„Éº„Çø„Å´„Å§„Åç 0.4„Ç§„É≥„ÉÅ„ÅÆÈ´ò„Åï„ÇíÁ¢∫‰øù„Åô„ÇãÔºàÊúÄ‰Ωé„Åß„ÇÇ6„Ç§„É≥„ÉÅÔºâ
                    dynamic_height = max(6, len(words) * 0.4)
                    
                    fig, ax = plt.subplots(figsize=(10, dynamic_height))
                    ax.barh(words, counts, color='skyblue')
                    ax.invert_yaxis() # ‰∏ä‰Ωç„Çí‰∏ä„Å´
                    
                    # „Ç∞„É™„ÉÉ„ÉâÁ∑ö„Å™„Å©„ÇíÂÖ•„Çå„Å¶Ë¶ã„ÇÑ„Åô„Åè
                    ax.grid(axis='x', linestyle='--', alpha=0.7)
                    ax.set_title(f"Âá∫ÁèæÂõûÊï∞„É©„É≥„Ç≠„É≥„Ç∞ (TOP {top_n})")
                    
                    st.pyplot(fig)
                else:
                    st.warning("ÂàÜÊûê„Åß„Åç„ÇãÂçòË™û„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ")

    with tab3:
        st.subheader("„ÉØ„Éº„Éâ„ÇØ„É©„Ç¶„Éâ")
        if st.button("‰ΩúÊàê„Åô„Çã", key='btn_wc'):
            with st.spinner("ÊèèÁîª‰∏≠..."):
                text_data = " ".join(df[target_col].dropna().astype(str).tolist())
                tokens = get_tokens(text_data, DEFAULT_STOPWORDS)
                text_space_sep = " ".join(tokens)
                try:
                    wc = WordCloud(
                        background_color="white", width=800, height=500,
                        regexp=r"[\w']+", font_path="IPAexGothic.ttf"
                    ).generate(text_space_sep)
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis("off")
                    st.pyplot(fig)
                except Exception as e:
                    st.error("„Ç®„É©„Éº: „Éï„Ç©„É≥„Éà„Éï„Ç°„Ç§„É´„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

    with tab4:
        st.subheader("ÂÖ±Ëµ∑„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ")
        col1, col2 = st.columns(2)
        with col1:
            net_top_n = st.slider("„Ç®„ÉÉ„Ç∏Êï∞", 10, 200, 50, key='net_slider')
        with col2:
            min_edge = st.slider("ÊúÄÂ∞èÂÖ±Ëµ∑ÂõûÊï∞", 1, 10, 1, key='net_edge')

        if st.button("„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÇíË°®Á§∫", key='btn_net'):
            with st.spinner("ÊßãÁØâ‰∏≠..."):
                sentences = df[target_col].dropna().astype(str).tolist()
                tokens_list = [get_tokens(s, DEFAULT_STOPWORDS) for s in sentences]
                G = create_cooccurrence_network(tokens_list, top_n=net_top_n, min_edge_weight=min_edge)
                
                if G.number_of_nodes() > 0:
                    fig, ax = plt.subplots(figsize=(12, 12))
                    pos = nx.spring_layout(G, k=0.5, seed=42)
                    nx.draw_networkx_nodes(G, pos, node_size=300, node_color='#a0cbe2', alpha=0.9, ax=ax)
                    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color='gray', ax=ax)
                    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_size=11, ax=ax)
                    ax.axis('off')
                    st.pyplot(fig)
                else:
                    st.warning("„Å§„Å™„Åå„Çä„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇÊù°‰ª∂„ÇíÁ∑©„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ")
