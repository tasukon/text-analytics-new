import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from janome.tokenizer import Tokenizer
from collections import Counter
from wordcloud import WordCloud
import re
import time
import networkx as nx

# --- 1. ã‚¢ãƒ—ãƒªã®åŸºæœ¬è¨­å®š ---
st.set_page_config(
    page_title="Text Analytics App",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚’è¨˜æ†¶ã•ã›ã‚‹é‡‘åº«ã‚’ä½œã‚‹ï¼‰
if 'df' not in st.session_state:
    st.session_state.df = None

# ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã®å®šç¾©
DEFAULT_STOPWORDS = [
    "ã®", "ã«", "ã¯", "ã‚’", "ãŸ", "ãŒ", "ã§", "ã¦", "ã¨", "ã—", "ã‚Œ", "ã•",
    "ã‚ã‚‹", "ã„ã‚‹", "ã‚‚", "ã™ã‚‹", "ã‹ã‚‰", "ãª", "ã“ã¨", "ã¨ã—ã¦", "ã„", "ã‚„",
    "ã‚Œã‚‹", "ãªã©", "ãªã„", "ã“ã®", "ãŸã‚", "ãã®", "ã‚ˆã†", "ã¾ãŸ", "ã‚‚ã®",
    "ã¾ã™", "ã§ã™", "ã•ã‚“", "ã¡ã‚ƒã‚“", "ãã‚“"
]

# --- 2. é–¢æ•°ã®å®šç¾© ---

@st.cache_data
def get_tokens(text, stop_words):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©ãƒ»å‹•è©ãƒ»å½¢å®¹è©ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°"""
    t = Tokenizer()
    tokens = []
    if not isinstance(text, str):
        return []
    
    for token in t.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(',')[0]
        if pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and len(base) > 1 and base not in stop_words:
            tokens.append(base)
    return tokens

@st.cache_data
def create_demo_data():
    """ãƒ‡ãƒ¢ç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
    data = {
        'å­¦å¹´': ['1å¹´', '1å¹´', '2å¹´', '2å¹´', '3å¹´', '3å¹´', '1å¹´', '2å¹´', '3å¹´', '1å¹´'],
        'æ€§åˆ¥': ['ç”·æ€§', 'å¥³æ€§', 'ç”·æ€§', 'å¥³æ€§', 'ç”·æ€§', 'å¥³æ€§', 'å¥³æ€§', 'ç”·æ€§', 'å¥³æ€§', 'ç”·æ€§'],
        'è‡ªç”±è¨˜è¿°': [
            'é‡æ€§å‘³ã‚ãµã‚Œã‚‹äººæã«ãªã‚ŠãŸã„ã—ã€ä¾¡å€¤å‰µé€ ã‚‚é‡è¦ã ã¨æ€ã†ã€‚',
            'æ–°ã—ã„ä¾¡å€¤ã‚’ä½œã‚‹ãŸã‚ã«ã¯ã€é‡æ€§çš„ãªå‹˜ãŒå¿…è¦ã ã¨æ„Ÿã˜ã‚‹ã€‚',
            'å­¦æ ¡ç”Ÿæ´»ã§é‡æ€§å‘³ã‚’ç£¨ãã€ç¤¾ä¼šã§æ´»èºã—ãŸã„ã€‚',
            'ä¾¡å€¤å‰µé€ äººæã¨ã¯ã€å¤±æ•—ã‚’æã‚Œãšã«æŒ‘æˆ¦ã™ã‚‹äººã®ã“ã¨ã ã€‚',
            'å‹‰å¼·ã ã‘ã§ãªãã€éƒ¨æ´»å‹•ã§ã‚‚é‡æ€§å‘³ã‚’å‡ºã—ã¦ã„ããŸã„ã€‚',
            'å°†æ¥ã¯ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªä»•äº‹ã§ä¾¡å€¤ã‚’ç”Ÿã¿å‡ºã—ãŸã„ã€‚',
            'é‡æ€§å‘³ã¨ã¯ã€å›°é›£ã«ç«‹ã¡å‘ã‹ã†å¼·ã•ã®ã“ã¨ã ã¨æ€ã†ã€‚',
            'ä»²é–“ã¨å”åŠ›ã—ã¦æ–°ã—ã„ä¾¡å€¤ã‚’å‰µé€ ã™ã‚‹ã“ã¨ãŒç›®æ¨™ã§ã™ã€‚',
            'ã‚‚ã£ã¨è‡ªç”±ã«ã€é‡æ€§çš„ã«ç”Ÿãã¦ã„ããŸã„ã€‚',
            'ä¾¡å€¤å‰µé€ ã®ãŸã‚ã«ã¯ã€åŸºç¤çš„ãªçŸ¥è­˜ã‚‚å¤§åˆ‡ã ã€‚'
        ]
    }
    return pd.DataFrame(data)

# --- 3. ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆãƒ‡ãƒ¼ã‚¿å…¥åŠ›ï¼‰ ---
st.sidebar.title("ğŸ›  è¨­å®š & ãƒ‡ãƒ¼ã‚¿å…¥åŠ›")

# CSVã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’è¿½åŠ 
input_method = st.sidebar.radio("ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿æ–¹æ³•", ["ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†", "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURL", "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"])

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å‡¦ç† ---
# ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€session_state.df ã«ãƒ‡ãƒ¼ã‚¿ãŒä¿å­˜ã•ã‚Œã‚‹ä»•çµ„ã¿ã«å¤‰æ›´

if input_method == "ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã†":
    if st.sidebar.button("ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰"):
        st.session_state.df = create_demo_data()
        st.sidebar.success("ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")

elif input_method == "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURL":
    url = st.sidebar.text_input("ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®URL")
    if st.sidebar.button("ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€"):
        if url:
            try:
                with st.spinner("ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­..."):
                    match = re.search(r'/d/([a-zA-Z0-9-_]+)', url)
                    if match:
                        file_id = match.group(1)
                        csv_url = f'https://docs.google.com/spreadsheets/d/{file_id}/export?format=csv'
                        st.session_state.df = pd.read_csv(csv_url)
                        st.sidebar.success(f"èª­ã¿è¾¼ã¿æˆåŠŸï¼ ({len(st.session_state.df)}è¡Œ)")
                    else:
                        st.sidebar.error("URLã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚")
            except Exception as e:
                st.sidebar.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

elif input_method == "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
    uploaded_file = st.sidebar.file_uploader("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°&ãƒ‰ãƒ­ãƒƒãƒ—", type=['csv'])
    if uploaded_file is not None:
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‚‰ã™ãã«èª­ã¿è¾¼ã‚€
        try:
            st.session_state.df = pd.read_csv(uploaded_file)
            st.sidebar.success(f"èª­ã¿è¾¼ã¿æˆåŠŸï¼ ({len(st.session_state.df)}è¡Œ)")
        except Exception as e:
            st.sidebar.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# --- 4. ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("ğŸ“Š ãƒ†ã‚­ã‚¹ãƒˆåˆ†æã‚¢ãƒ—ãƒª")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆï¼ˆé‡‘åº«ï¼‰ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ç¢ºèª
if st.session_state.df is not None:
    df = st.session_state.df  # ä½¿ã„ã‚„ã™ã„ã‚ˆã†ã«å¤‰æ•°ã«å…¥ã‚Œã‚‹
    
    # ã‚¿ãƒ–ä½œæˆ
    tab1, tab2, tab3 = st.tabs(["ğŸ“‚ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç¢ºèª", "ğŸ“ˆ é »å‡ºå˜èªåˆ†æ", "â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰"])

    with tab1:
        st.header("èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿")
        st.dataframe(df)

    with tab2:
        st.header("é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°")
        # ã™ã¹ã¦ã®åˆ—ã‚’å€™è£œã«ã™ã‚‹ï¼ˆæ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚‚é¸ã¹ã‚‹ã‚ˆã†ã«ä¿®æ­£ï¼‰
        all_cols = df.columns
        target_col = st.selectbox("åˆ†æã™ã‚‹æ–‡ç« ã®åˆ—ã‚’é¸ã‚“ã§ãã ã•ã„", all_cols, index=len(all_cols)-1)
        
        top_n = st.slider("è¡¨ç¤ºã™ã‚‹å˜èªæ•°", 5, 50, 10)

        if st.button("ã‚°ãƒ©ãƒ•ã‚’è¡¨ç¤º"):
            with st.spinner("è§£æä¸­..."):
                # é¸ã‚“ã åˆ—ã‚’å¼·åˆ¶çš„ã«æ–‡å­—å‹(str)ã«å¤‰æ›ã—ã¦çµåˆ
                text_data = " ".join(df[target_col].dropna().astype(str).tolist())
                tokens = get_tokens(text_data, DEFAULT_STOPWORDS)
                
                if tokens:
                    counter = Counter(tokens)
                    words, counts = zip(*counter.most_common(top_n))
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.barh(words, counts, color='skyblue')
                    ax.invert_yaxis()
                    ax.set_title(f"ã€Œ{target_col}ã€ã®é »å‡ºå˜èª TOP{top_n}")
                    st.pyplot(fig)
                else:
                    st.warning("åˆ†æå¯èƒ½ãªå˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    with tab3:
        st.header("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
        target_col_wc = st.selectbox("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã«ã™ã‚‹åˆ—", all_cols, key='wc_select')
        
        if st.button("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ä½œæˆ"):
            with st.spinner("æç”»ä¸­..."):
                text_data = " ".join(df[target_col_wc].dropna().astype(str).tolist())
                tokens = get_tokens(text_data, DEFAULT_STOPWORDS)
                text_space_sep = " ".join(tokens)
                
                try:
                    wc = WordCloud(
                        background_color="white",
                        width=800, height=500,
                        regexp=r"[\w']+",
                        font_path="IPAexGothic.ttf"
                    ).generate(text_space_sep)
                    
                    fig_wc, ax_wc = plt.subplots(figsize=(12, 8))
                    ax_wc.imshow(wc, interpolation='bilinear')
                    ax_wc.axis("off")
                    st.pyplot(fig_wc)
                except Exception as e:
                    st.error("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    st.write(e)

else:
    st.info("ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„")
