import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import japanize_matplotlib
from janome.tokenizer import Tokenizer
from collections import Counter
from wordcloud import WordCloud
import networkx as nx
import itertools
import re
import time

# --- 1. ã‚¢ãƒ—ãƒªã®è¨­å®š ---
st.set_page_config(page_title="Text Analytics V6", layout="wide")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆåˆæœŸåŒ–
if 'df' not in st.session_state:
    st.session_state.df = None
if 'user_stopwords' not in st.session_state:
    st.session_state.user_stopwords = []
if 'step' not in st.session_state:
    st.session_state.step = 1

# åŸºæœ¬ã®ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰
DEFAULT_STOPWORDS = [
    "ã®", "ã«", "ã¯", "ã‚’", "ãŸ", "ãŒ", "ã§", "ã¦", "ã¨", "ã—", "ã‚Œ", "ã•",
    "ã‚ã‚‹", "ã„ã‚‹", "ã‚‚", "ã™ã‚‹", "ã‹ã‚‰", "ãª", "ã“ã¨", "ã¨ã—ã¦", "ã„", "ã‚„",
    "ã‚Œã‚‹", "ãªã©", "ãªã„", "ã“ã®", "ãŸã‚", "ãã®", "ã‚ˆã†", "ã¾ãŸ", "ã‚‚ã®",
    "ã¾ã™", "ã§ã™", "ã•ã‚“", "ã¡ã‚ƒã‚“", "ãã‚“", "ã‚ã£", "ã‚ã‚Š", "ã„ã£", "ã†",
    "ã‹", "ã›ã‚‹", "ãŸã„", "ã ã‘", "ãŸã¡", "ã¤ã„ã¦", "ã§ã", "ãªã‚Š", "ã®",
    "ã°ã‹ã‚Š", "ã»ã©", "ã¾ã§", "ã¾ã¾", "ã‚ˆã†", "ã‚ˆã‚Š", "ã‚ãŸã—", "ãã‚Œ", "ã“ã‚Œ",
    "å›ç­”", "ãªã—", "ç‰¹ã«ãªã—", "ç‰¹ã«", "ãŸã‚", "ã¦ã", "ãã‚Œã‚‰"
]

# --- 2. é–¢æ•°å®šç¾© ---

def classify_columns(df):
    """å±æ€§(ãƒ•ã‚£ãƒ«ã‚¿ç”¨)ã¨ãƒ†ã‚­ã‚¹ãƒˆ(åˆ†æç”¨)ã‚’è‡ªå‹•åˆ¤å®š"""
    filter_cols = [] 
    text_cols = []   

    for col in df.columns:
        unique_count = df[col].nunique()
        # 50ç¨®é¡æœªæº€ãªã‚‰ã€Œå±æ€§ã€ã¨ã¿ãªã™
        if unique_count < 50:
            filter_cols.append(col)
        # ãã‚Œä»¥å¤–ã§æ–‡å­—å‹ãªã‚‰ã€Œãƒ†ã‚­ã‚¹ãƒˆã€ã¨ã¿ãªã™
        elif df[col].dtype == 'object':
            text_cols.append(col)
            
    return filter_cols, text_cols

@st.cache_data
def get_tokens(text, stop_words):
    """å½¢æ…‹ç´ è§£æ"""
    t = Tokenizer()
    tokens = []
    if not isinstance(text, str):
        return []
    
    japanese_pattern = re.compile(r'[ã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]')
    
    for token in t.tokenize(text):
        base = token.base_form
        pos = token.part_of_speech.split(',')[0]
        if (pos in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and 
            len(base) > 1 and 
            base not in stop_words and 
            japanese_pattern.search(base)):
            tokens.append(base)
    return tokens

@st.cache_data
def create_network(tokens_list, top_n, min_edge):
    """å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”Ÿæˆ"""
    pair_list = []
    for tokens in tokens_list:
        if len(tokens) >= 2:
            pair_list.extend(itertools.combinations(tokens, 2))
    
    c = Counter(pair_list)
    top_pairs = c.most_common(top_n)
    
    G = nx.Graph()
    for (u, v), weight in top_pairs:
        if weight >= min_edge:
            G.add_edge(u, v, weight=weight)
    return G

# --- 3. ãƒ¡ã‚¤ãƒ³å‡¦ç† ---

# === STEP 1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ ===
if st.session_state.step == 1:
    st.title("ğŸ“‚ Step 1: ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿")
    st.info("åˆ†æã—ãŸã„ CSV ã¾ãŸã¯ Excel ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    
    uploaded_file = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—", type=['csv', 'xlsx'])
    
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            st.session_state.df = df
            st.session_state.step = 2
            st.rerun()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

# === STEP 2: é™¤å¤–ãƒ¯ãƒ¼ãƒ‰è¨­å®š (å…¨ãƒ†ã‚­ã‚¹ãƒˆä¸€æ‹¬) ===
elif st.session_state.step == 2:
    
    # --- ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ: ã‚¿ã‚¤ãƒˆãƒ«ã¨ã€Œæ¬¡ã¸ã€ãƒœã‚¿ãƒ³ã‚’æ¨ªä¸¦ã³ã« ---
    header_col1, header_col2 = st.columns([3, 1])
    with header_col1:
        st.title("ğŸ§¹ Step 2: ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°")
        st.markdown("ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®**ã™ã¹ã¦ã®æ–‡ç« **ã‹ã‚‰ã€é »å‡ºå˜èªã‚’é›†è¨ˆã—ã¾ã—ãŸã€‚ä¸è¦ãªè¨€è‘‰ã‚’é™¤å¤–ã—ã¦ãã ã•ã„ã€‚")
    
    df = st.session_state.df
    filter_candidates, text_candidates = classify_columns(df)
    
    # å®Œäº†ãƒœã‚¿ãƒ³ï¼ˆä¸Šéƒ¨ã«é…ç½®ï¼‰
    with header_col2:
        st.write("") # ä½™ç™½èª¿æ•´
        if st.button("è¨­å®šå®Œäº†ï¼åˆ†æç”»é¢ã¸ (Step 3) >>", type="primary", use_container_width=True):
            st.session_state.text_candidates = text_candidates # è‡ªå‹•åˆ¤å®šã—ãŸãƒ†ã‚­ã‚¹ãƒˆåˆ—
            st.session_state.filter_candidates = filter_candidates # è‡ªå‹•åˆ¤å®šã—ãŸå±æ€§åˆ—
            st.session_state.step = 3
            st.rerun()

    st.markdown("---")

    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("å…¨ä½“ãƒ©ãƒ³ã‚­ãƒ³ã‚° (TOP30)")
        
        # å…¨ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’çµåˆã—ã¦åˆ†æ
        current_stop = DEFAULT_STOPWORDS + st.session_state.user_stopwords
        
        full_text_data = ""
        # è‡ªå‹•åˆ¤å®šã•ã‚ŒãŸã€Œãƒ†ã‚­ã‚¹ãƒˆåˆ—ã€ã®ä¸­èº«ã‚’å…¨éƒ¨ã¤ãªã’ã‚‹
        target_cols = text_candidates if text_candidates else df.columns
        
        for col in target_cols:
            full_text_data += " " + " ".join(df[col].dropna().astype(str).tolist())
        
        tokens = get_tokens(full_text_data, current_stop)
        
        if tokens:
            c = Counter(tokens)
            words, counts = zip(*c.most_common(30))
            
            # ã‚°ãƒ©ãƒ•æç”»
            fig, ax = plt.subplots(figsize=(6, 8))
            ax.barh(words, counts, color='gray')
            ax.invert_yaxis()
            ax.set_title("ã™ã¹ã¦ã®æ–‡ç« ã®åˆè¨ˆ")
            st.pyplot(fig)
        else:
            st.warning("ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    with col2:
        st.subheader("é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã®è¿½åŠ ")
        st.info("å·¦ã®ã‚°ãƒ©ãƒ•ã‚’è¦‹ã¦ã€åˆ†æã«ä¸è¦ãªå˜èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        new_word = st.text_input("é™¤å¤–ã—ãŸã„å˜èª (å…¥åŠ›ã—ã¦Enter)", placeholder="ä¾‹: ç§ æ€ã† ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆ")
        if new_word:
            words = new_word.split()
            added = []
            for w in words:
                if w not in st.session_state.user_stopwords:
                    st.session_state.user_stopwords.append(w)
                    added.append(w)
            if added:
                st.success(f"é™¤å¤–ã—ã¾ã—ãŸ: {added}")
                time.sleep(0.5)
                st.rerun()
        
        st.write("ğŸš« **ç¾åœ¨ã®é™¤å¤–ãƒªã‚¹ãƒˆ:**")
        st.write(st.session_state.user_stopwords)
        
        if st.button("ãƒªã‚»ãƒƒãƒˆ"):
            st.session_state.user_stopwords = []
            st.rerun()

# === STEP 3: æœ€çµ‚åˆ†æ (å¤šé‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° & å¯è¦–åŒ–) ===
elif st.session_state.step == 3:
    st.title("ğŸ“Š Step 3: è©³ç´°åˆ†æ")
    
    df = st.session_state.df
    text_candidates = st.session_state.text_candidates
    filter_candidates = st.session_state.filter_candidates
    stop_words = DEFAULT_STOPWORDS + st.session_state.user_stopwords

    # --- ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š ---
    st.sidebar.header("ğŸ” ãƒ‡ãƒ¼ã‚¿ã®çµã‚Šè¾¼ã¿")
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†
    df_filtered = df.copy()
    
    # è‡ªå‹•åˆ¤å®šã•ã‚ŒãŸå±æ€§åˆ—ã”ã¨ã«ã€é¸æŠãƒœãƒƒã‚¯ã‚¹ã‚’ä½œã‚‹
    active_filters = []
    for col in filter_candidates:
        unique_vals = sorted(df[col].dropna().unique().tolist())
        selected = st.sidebar.multiselect(f"{col}", unique_vals)
        
        if selected:
            df_filtered = df_filtered[df_filtered[col].isin(selected)]
            
    st.sidebar.markdown("---")
    st.sidebar.write(f"**åˆ†æå¯¾è±¡:** {len(df_filtered)} è¡Œ / {len(df)} è¡Œ")
    
    # æˆ»ã‚‹ãƒœã‚¿ãƒ³
    if st.sidebar.button("Step 2 (é™¤å¤–è¨­å®š) ã«æˆ»ã‚‹"):
        st.session_state.step = 2
        st.rerun()
    if st.sidebar.button("Step 1 (ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ) ã«æˆ»ã‚‹"):
        st.session_state.df = None
        st.session_state.step = 1
        st.rerun()

    # --- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢: å¯è¦–åŒ– ---
    
    if len(df_filtered) == 0:
        st.error("æ¡ä»¶ã«åˆã†ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã§ã™ã€‚ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ç·©ã‚ã¦ãã ã•ã„ã€‚")
    else:
        # Step 3ã§ã¯ã€å¯¾è±¡ã¨ãªã‚‹å…¨ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’çµåˆã—ã¦è¡¨ç¤º
        full_text = ""
        target_cols = text_candidates if text_candidates else df.columns
        for col in target_cols:
            full_text += " " + " ".join(df_filtered[col].dropna().astype(str).tolist())
            
        tokens = get_tokens(full_text, stop_words)

        if not tokens:
            st.warning("è¡¨ç¤ºã§ãã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        else:
            tab1, tab2, tab3 = st.tabs(["â˜ï¸ ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰", "ğŸ•¸ï¸ å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯", "ğŸ“ˆ ãƒ©ãƒ³ã‚­ãƒ³ã‚°"])
            
            with tab1:
                st.subheader("ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰")
                try:
                    wc = WordCloud(
                        background_color="white", width=800, height=500,
                        regexp=r"[\w']+", font_path="IPAexGothic.ttf"
                    ).generate(" ".join(tokens))
                    fig, ax = plt.subplots(figsize=(10, 6))
                    ax.imshow(wc, interpolation='bilinear')
                    ax.axis("off")
                    st.pyplot(fig)
                except:
                    st.error("ãƒ•ã‚©ãƒ³ãƒˆèª­è¾¼ã‚¨ãƒ©ãƒ¼")

            with tab2:
                st.subheader("å…±èµ·ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯")
                col1, col2 = st.columns(2)
                with col1:
                    net_top = st.slider("ã‚¨ãƒƒã‚¸æ•°", 10, 200, 50, key='net1')
                with col2:
                    min_edge = st.slider("æœ€å°å…±èµ·å›æ•°", 1, 10, 2, key='net2')
                
                # è¡Œã”ã¨ã®ãƒªã‚¹ãƒˆä½œæˆï¼ˆå…¨ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’çµåˆï¼‰
                sentences = []
                for i, row in df_filtered.iterrows():
                    row_text = " ".join([str(row[c]) for c in target_cols if pd.notna(row[c])])
                    sentences.append(row_text)

                tokens_list = [get_tokens(s, stop_words) for s in sentences]
                G = create_network(tokens_list, net_top, min_edge)
                
                if G.number_of_nodes() > 0:
                    fig, ax = plt.subplots(figsize=(10, 10))
                    pos = nx.spring_layout(G, k=0.6, seed=42)
                    nx.draw_networkx_nodes(G, pos, node_size=300, node_color='#66b3ff', alpha=0.9, ax=ax)
                    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5, edge_color='gray', ax=ax)
                    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic', font_size=11, ax=ax)
                    ax.axis('off')
                    st.pyplot(fig)
                else:
                    st.warning("ã¤ãªãŒã‚ŠãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

            with tab3:
                st.subheader("é »å‡ºå˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°")
                c = Counter(tokens)
                common = c.most_common(20)
                words, counts = zip(*common)
                fig, ax = plt.subplots(figsize=(8, 6))
                ax.barh(words, counts, color='skyblue')
